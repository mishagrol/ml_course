{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "md_Intro",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"skoltech_logo.png\" alt=\"Skoltech\" width=60% height=60% />\n",
    "<h1 style=\"color:#333333; text-align:center; line-height: 0;\">Reinforcement Learning</h1>\n",
    "<h5 style=\"color:#333333; text-align:center;\">Course MA030422</h5>\n",
    "\n",
    "<h2 style=\"color:#A7BD3F;\">Lab 1</h2>\n",
    "\n",
    "***\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Welcome to the first lab of the *Reinforcement Learning* course at Skoltech CDISE! \n",
    "\n",
    "After the first two lectures and seminars of this course, you have learned about several foundational concepts in reinforcement learning. These concepts include:\n",
    "\n",
    "* The environment (a.k.a \"system\" in control theory language)\n",
    "* Agent (a.k.a \"controller\")\n",
    "* States and actions (state-space and action-space)\n",
    "* Disturbance\n",
    "* Discrete vs Continuous Time\n",
    "* Markov Decision Processes (MDP)\n",
    "* Policy: deterministic and stochastic policies\n",
    "* Objective functions: *reward* function and *cost-to-go* function (frequently known as the **Bellman value function**)\n",
    "* Optimization of the objective functions\n",
    "* Discounting of rewards\n",
    "* Time horizons: finite or infinite\n",
    "* The overall goal of the agent: to learn the **optimal** implicit or explicit model of the environment in order act optimally to fulfill some goal.\n",
    "\n",
    "\n",
    "### Why labs?\n",
    "\n",
    "Lab assignments offer various benefits to students. Mainly, they a) provide students with an opportunity to *apply* the theoretical concepts and methods that they learned about during lecture; b) teach students how to use technology related to the course; and c) give students an opportunity for problem-solving.\n",
    "\n",
    "### Goal of this lab\n",
    "\n",
    "The purpose of this lab **<font color=\"green\">is</font>**:\n",
    "* to become acquainted with the basic functionality of a Python package that we will be utilizing for learning RL.\n",
    "\n",
    "The purpose of this lab **<font color=\"red\">is not</font>**:\n",
    "* to dive into detailed RL algorithms (this comes later)\n",
    "\n",
    "### Components of this lab (Total points: 15)\n",
    "* **Section 1**: Prerequisites\n",
    "* **Section 2**: Intro to Rcognita\n",
    "* **Section 3**: Using Rcognita\n",
    "    * Exercise 1\n",
    "        * Problem 1.1 (1 point)\n",
    "        * Problem 1.2 (1 point)\n",
    "    * Exercise 2\n",
    "        * Problem 2.1 (3 points)\n",
    "        * Problem 2.2 (5 points)\n",
    "        * Problem 2.3 (2 points)\n",
    "        * Problem 2.4 (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "md_Section_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h2 style=\"color:#A7BD3F;\">Section 1 - Prerequisites</h2>\n",
    "\n",
    "***\n",
    "\n",
    "### Before getting started: installation\n",
    "\n",
    "To complete this lab, you first need to install *Rcognita*. You should have already done this after reading the repo README - but if you haven't then head over to Canvas and read the installation_requirements document. Note that it is a best practice to install this package into a virtual environment.\n",
    "\n",
    "### Quick review\n",
    "\n",
    "Thus far, you have learned about the two principle entities in reinforcement learning: the agent and the environment. At a basic level, the interaction between these two entities is as follows: the agent observes the state of the environment, calculcates and takes actions, and receives rewards and new states from the environment. This is a cyclical process <sup>[1]</sup>:\n",
    "\n",
    "<img src=\"agent_env.png\" alt=\"Agent-Env interaction\" width=55% height=55% />\n",
    "\n",
    "Through this process, the agent learns (an implicit or explicit policy) for taking actions that maximize reward over some time horizon.\n",
    "\n",
    "#### Before we get started, a word on terminology\n",
    "\n",
    "If you had a chance to dive into the Python code of this package on your own time, then you might have noticed some terminology that differs from more \"popular\" RL terms. I.e., you may have seen:\n",
    "* \"System\" instead of environment\n",
    "* \"Controller\" instead of agent\n",
    "* \"cost-to-go\" instead of optimal (Bellman) value function\n",
    "* \"Feedback loop\" instead of training iteration\n",
    "\n",
    "These terms are interrelated and often interchangeable. They come from systems theory, control theory, and dynamic programming. RL has evolved from these fields (among others) and thus shares many concepts with them, especially as related to mathematical processes and entity names. \n",
    "\n",
    "For our purposes, you should become acquainted with the following terms and concepts <sup>[2]</sup>:\n",
    "\n",
    "<img src=\"notation.png\" alt=\"Notation\" width=45% height=45% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "md_Section_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h2 style=\"color:#A7BD3F;\">Section 2 - Intro to Rcognita</h2>\n",
    "\n",
    "***\n",
    "\n",
    "### Rcognita\n",
    "\n",
    "Rcognita is Python framework for RL training, research and visualization. The purpose of Rcognita is to train RL algorithms in a dynamic environment.\n",
    "* **Definition**: An environment is dynamic if it changes while the agent is in the process of action selection. Otherwise, the environment is static.\n",
    "\n",
    "The **environment** in Rcognita is based upon a 3-wheel robot:\n",
    "\n",
    "<img src=\"endi1.png\" alt=\"Notation\" width=40% height=40% />\n",
    "\n",
    "The purpose of the **agent** is to learn to control the robot to move it to the target (0,0) xy-coordinates 2D Euclidean space:\n",
    "\n",
    "<img src=\"endi2.png\" alt=\"Notation\" width=40% height=40% />\n",
    "\n",
    "This training task and objective can be achieved with a variety of agent algorithms and customizations to the environment, which we will discuss later.\n",
    "\n",
    "### Environment inputs and outputs\n",
    "\n",
    "Next, let's examine the input/output variables of the agent and environment in Rcognita in order to understand how they interact.\n",
    "\n",
    "The environment (being the robot in Euclidean space), takes two inputs:\n",
    "* **F**: pushing force (N for Newton)\n",
    "* **M**: turning torque (Nm for Newton metre)\n",
    "\n",
    "It is the agent's task to determine these two inputs and input them to the environment. \n",
    "\n",
    "For it's part, the environment outputs a new state, which has the following 5 characteristics:\n",
    "* **x ($x_c$)** = x-coordinate (m)\n",
    "* **y ($y_c$)** = y-coordinate (m)\n",
    "* **alpha ($\\alpha$)** = turning angle (rad)\n",
    "* **upsilon ($\\upsilon$)** = velocity (m/s)\n",
    "* **omega ($\\omega$)** = revolution speed (rad/s) (aka turning speed)\n",
    "\n",
    "To calculate $\\upsilon$ (velocity) and $\\omega$ (revolution speed), the environment also takes the following 2 parameters:\n",
    "* **m** = robot's mass (kg)\n",
    "* **I** = robot's moment of inertia around the vertical axis (kg m^2)\n",
    "\n",
    "Let us now formulate these variables in a mathematical representation often used in control theory. For a given time-step, $t$:\n",
    "* Action or control input is given by: $\\hspace{3mm}u_t = (F, M)$\n",
    "* Environment is given by: $\\hspace{3mm}x_t = (x_c, y_c, \\alpha, \\upsilon, \\omega)$\n",
    "\n",
    "### The agent: learning how to act\n",
    "\n",
    "To understand how the agent learns to pilot a 3-wheeled robot to its objective, we can study a simplified Markov Decision Process (MDP). This MDP is \"simplified\" because it is formalized from the perspective of a *deterministic* environment (instead of a *stochastic* environment).\n",
    "* **Definition**: a deterministic environment is one in which a specific action in a given state always leads to the same next state. <sup>[3]</sup>\n",
    "\n",
    "In the case of a deterministic setting, the MDP is characterised by 3 functions:\n",
    "* The transition function: $f: X \\times U \\rightarrow U$\n",
    "* The reward function (Greek letter rho): $\\rho: X \\times U \\rightarrow R$\n",
    "* And the policy function $h: X \\rightarrow U$\n",
    "\n",
    "By taking the variables defined above and applying them to these functions, below we arrive at the following simplified MDP that governs training in Rcognita.\n",
    "\n",
    "The environment executes the transition function:\n",
    "\n",
    "$$x_{t+1} = f(x_t, u_t)$$\n",
    "\n",
    "For the current time-step $t$: the current state $x_t$ and controller action $u_t$ is inputted to the environment, which outputs the next state $x_{t+1}$.\n",
    "\n",
    "Similarly, the environment also provides a signal to the agent in the form of a reward:\n",
    "\n",
    "$$R_{t+1} = \\rho(x_t, u_t)$$\n",
    "\n",
    "The reward function is the objective function that the agent seeks to maximize. (**Note**: an RL training problem may be formulized as *minimizing* the objective function, where the objective is the \"cost-to-go\". This cost-to-go is the difference between the optimal value function and the agent's *learned* value function. That is why the term cost-to-go is used and in this case we are minimizing, not maximizing).\n",
    "\n",
    "And finally, the policy function. The policy is precisely what the agent needs to learn to succeed at the objective - and it is described as:\n",
    "\n",
    "$$ u_t = h(x_t) $$\n",
    "\n",
    "Thus at time step $t$, the agent takes as input the state of the environment $x_t$ and outputs the action to take, $u_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "md_Section_3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h2 style=\"color:#A7BD3F;\">Section 3 - Using Rcognita</h2>\n",
    "\n",
    "***\n",
    "\n",
    "### Design of *Rcognita*\n",
    "\n",
    "Rcognita contains 4 *primary* classes:\n",
    "\n",
    "1. System class\n",
    "    - a.k.a environment\n",
    "    - key functionality: defines the state of the environment and returns the next state given an action from the controller\n",
    "2. Controller class\n",
    "    - a.k.a agent\n",
    "    - key functionality: implements ActorCritic model that calculates actions for states\n",
    "    - Goal is to learn a model for taking actions (aka policy) that minimizes the cost-to-go objective\n",
    "3. NominalController class\n",
    "    - Calculates actions that serve as a baseline for comparison to the controller's actions\n",
    "4. Simulation class\n",
    "    - Creates and runs a training simulation\n",
    "    \n",
    "Let's examine these classes in code and learn how to use Rcognita.\n",
    "\n",
    "### First, import the package. \n",
    "<font color=\"red\">‚ùó**Note**:</font> make sure that the kernel selected in Jupyter (labeled in the top-right corner under the logout button in this notebook) is the virtual env that you installed Rcognita into! You can change this under the Kernel task bar. See the lab_1 README on github for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "code_s1_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from rcognita import System, NominalController, Controller, Simulation, AnswerTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading: run answer tracker</font>\n",
    "Run this cell to track your answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADING DO NOT MODIFY\n",
    "lab1_answers = AnswerTracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "md_s1_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h2 style=\"color:blue;\">Exercise 1 - reading the docs</h2> \n",
    "\n",
    "Much of the initial documentation for this package is baked right into the code in the form of *docstrings*. Examining docstrings is a way of reading documentation about each class and learning the components of functionality.\n",
    "\n",
    "Let's study the System class by calling the `print_docstring` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "code_s1_3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Class denoting the RL environment.\n",
      "\n",
      "    ----------\n",
      "    Parameters\n",
      "    ----------\n",
      "\n",
      "    dim_state : int\n",
      "        dimension of state vector\n",
      "        x_t = [x_c, y_c, alpha, upsilon, omega]\n",
      "\n",
      "    dim_input : int\n",
      "        * dimension of action vector\n",
      "        * u_t = [F, M]\n",
      "\n",
      "    dim_output : int\n",
      "        * dimension of output vector\n",
      "        * x_t+1 = [x_c, y_c, alpha, upsilon, omega]\n",
      "\n",
      "    dim_disturb : int\n",
      "        * dimension of disturbance vector\n",
      "        * actuator disturbance that gets added to F and M\n",
      "\n",
      "    initial_x : int\n",
      "        * initial x coordinate of robot\n",
      "\n",
      "    initial_y : int\n",
      "        * initial x coordinate of robot\n",
      "\n",
      "    m : int\n",
      "        * m = robot's mass\n",
      "\n",
      "    I : int\n",
      "        * I = moment of inertia about the vertical axis\n",
      "\n",
      "    f_min, f_max, m_min, m_max : all int\n",
      "        * control bounds\n",
      "\n",
      "    f_man : int\n",
      "        * manual control variable for pushing force\n",
      "\n",
      "    m_man: int\n",
      "        * manual control variable for steering/turning torque\n",
      "\n",
      "    is_dyn_ctrl : int\n",
      "        * is dynamic control?\n",
      "        * If 1, the controller (a.k.a. agent) is considered as a part of the full state vector\n",
      "\n",
      "    is_disturb : int\n",
      "        * use disturbance?\n",
      "        * If 0, no disturbance is fed into the system\n",
      "\n",
      "    sigma_q, mu_q, tau_q : int\n",
      "        * hyperparameters to disturbance\n",
      "\n",
      "    ----------\n",
      "    Attributes\n",
      "    ----------\n",
      "\n",
      "    alpha : float\n",
      "        * turning angle\n",
      "\n",
      "    system_state\n",
      "        * state of the environment\n",
      "        * can be a vector or matrix (if there are multiple sub-states, i.e. for multiple controllers)\n",
      "\n",
      "    _dim_initial_full_state : int vector\n",
      "        * dimensions of full state\n",
      "\n",
      "    full_state : int vector\n",
      "        * includes the system state vector, control input vector and disturbance vector\n",
      "        * can be a vector or matrix (if there are multiple sub-states, i.e. for multiple controllers)\n",
      "\n",
      "    u0 : float vector\n",
      "        * control input vector\n",
      "\n",
      "    q0 : float vector\n",
      "        * disturbance vector\n",
      "\n",
      "    num_controllers : int\n",
      "        * number of controllers to be used with the environment\n",
      "\n",
      "    multi_sim : int\n",
      "        * used with multiple controllers (num_controllers > 1)\n",
      "        * variable used for the closed_loop function\n",
      "        * specifies that the closed_loop is being executed for a specific controller\n",
      "\n",
      "    ---------------------\n",
      "    Environment variables\n",
      "    ---------------------\n",
      "\n",
      "    x_—Å : x-coordinate [m]\n",
      "    \n",
      "    y_—Å : y-coordinate [m]\n",
      "    \n",
      "    alpha : turning angle [rad]\n",
      "    \n",
      "    v : speed [m/s]\n",
      "    \n",
      "    omega : revolution speed [rad/s]\n",
      "    \n",
      "    F : pushing force [N]\n",
      "    \n",
      "    M : steering torque [Nm]\n",
      "    \n",
      "    m : robot mass [kg]\n",
      "    \n",
      "    I : robot moment of inertia around vertical axis [kg m^2]\n",
      "    \n",
      "    q  : actuator disturbance\n",
      "\n",
      "\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "System.print_docstring()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "md_s1_4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Review this docstring and compare it to the information about environment variables from **Section 2**. Make sure you are familiar with the variables under the \"ENVIRONMENT VARIABLES\" table above.\n",
    "\n",
    "### View default parameters\n",
    "\n",
    "You can also view parameters and their default values for any class in *Rcognita* by calling the `print_init_params` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "code_s1_5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_state=5\n",
      "dim_input=2\n",
      "dim_output=5\n",
      "dim_disturb=2\n",
      "initial_x=5\n",
      "initial_y=5\n",
      "m=10\n",
      "I=1\n",
      "f_man=-3\n",
      "m_man=-1\n",
      "f_min=-5\n",
      "f_max=5\n",
      "m_min=-1\n",
      "m_max=1\n",
      "is_dyn_ctrl=0\n",
      "is_disturb=0\n",
      "sigma_q=None\n",
      "mu_q=None\n",
      "tau_q=None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "System.print_init_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "md_s1_6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h2 style=\"color:blue;\">Problem 1.1</h2>\n",
    "\n",
    "#### üéØ Task: Between the comments below, call the `print_docstring` method on the `Controller` class and briefly glance over the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Problem_1-1_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Optimal controller (a.k.a. agent) class.\n",
      "\n",
      "    ----------\n",
      "    Parameters\n",
      "    ----------\n",
      "\n",
      "    system : object of type `System` class\n",
      "        object of type System (class)\n",
      "\n",
      "    t0 : int\n",
      "        * Initial value of the controller's internal clock\n",
      "\n",
      "    t1 : int\n",
      "        * End value of controller's internal clock\n",
      "\n",
      "    n_actor : int\n",
      "        Number of prediction steps. n_actor=1 means the controller is purely data-driven and doesn't use prediction.\n",
      "\n",
      "    n_critic : int\n",
      "        Critic stack size. The critic optimizes the temporal error, a.k.a. the value (of state) function. The temporal errors are stacked up using the said buffer.\n",
      "\n",
      "    buffer_size : int\n",
      "        The size of the buffer to store data for model estimation. The bigger the buffer, the more accurate the estimation may be achieved. Using a larger buffer results in better model estimation at the expense of computational cost.\n",
      "\n",
      "    ctrl_mode : int\n",
      "        Modes with online model estimation are experimental\n",
      "        * 0 : manual constant control (only for basic testing)\n",
      "        * -1 : nominal parking controller (for benchmarking optimal controllers)\n",
      "        * 1 : model-predictive control (MPC). Prediction via discretized true model\n",
      "        * 2 : adaptive MPC. Prediction via estimated model\n",
      "        * 3 : RL: Q-learning with n_critic roll-outs of running cost. Prediction via discretized true model\n",
      "        * 4 : RL: Q-learning with n_critic roll-outs of running cost. Prediction via estimated model\n",
      "        * 5 : RL: stacked Q-learning. Prediction via discretized true model\n",
      "        * 6 : RL: stacked Q-learning. Prediction via estimated model\n",
      "\n",
      "        * Modes 1, 3, 5 use model for prediction, passed into class exogenously. This could be, for instance, a true system model\n",
      "        * Modes 2, 4, 6 use an estimated online\n",
      "\n",
      "    critic_mode : int\n",
      "        Choice of the structure of the critic's feature vector\n",
      "        * 1 - Quadratic-linear\n",
      "        * 2 - Quadratic\n",
      "        * 3 - Quadratic, no mixed terms\n",
      "        * 4 - Quadratic, no mixed terms in input and output\n",
      "\n",
      "    critic_update_time : float\n",
      "        * Time between critic updates\n",
      "\n",
      "    r_cost_struct : int\n",
      "        * Choice of the running cost structure. A typical choice is quadratic of the form [y, u].T * R1 [y, u], where R1 is the (usually diagonal) parameter matrix. For different structures, R2 is also used.\n",
      "        * 1 - quadratic chi.T @ R1 @ chi\n",
      "        * 2 - 4th order chi**2.T @ R2 @ chi**2 + chi.T @ R2 @ chi\n",
      "\n",
      "    sample_time : int or float\n",
      "        Controller's sampling time (in seconds). The system itself is continuous as a physical process while the controller is digital.\n",
      "        * the higher the sampling time, the more chattering in the control might occur. It even may lead to instability and failure to park the robot\n",
      "        * smaller sampling times lead to higher computation times\n",
      "        * especially controllers that use the estimated model are sensitive to sampling time, because inaccuracies in estimation lead to problems when propagated over longer periods of time. Experiment with sample_time and try achieve a trade-off between stability and computational performance\n",
      "\n",
      "    pred_step_size : float\n",
      "        * Prediction step size in `J` (in seconds). Is the time between the computation of control inputs and outputs J. Should be a multiple of `sample_time`.\n",
      "\n",
      "    estimator_buffer_fill : int\n",
      "        * Initial phase to fill the estimator's buffer before applying optimal control (in seconds)\n",
      "\n",
      "    estimator_buffer_power : int\n",
      "        * Power of probing noise during an initial phase to fill the estimator's buffer before applying optimal control\n",
      "\n",
      "    estimator_update_time : float\n",
      "        * In seconds, the time between model estimate updates. This constant determines how often the estimated parameters are updated. The more often the model is updated, the higher the computational burden is. On the other hand, more frequent updates help keep the model actual.\n",
      "\n",
      "    stacked_model_params : int\n",
      "        * Estimated model parameters can be stored in stacks and the best among the `stacked_model_params` last ones is picked.\n",
      "        * May improve the prediction quality somewhat\n",
      "\n",
      "    model_order : int\n",
      "        * The order of the state-space estimation model. We are interested in adequate predictions of y under given u's. The higher the model order, the better estimation results may be achieved, but be aware of overfitting.\n",
      "\n",
      "    gamma : float\n",
      "        * Discounting factor\n",
      "        * number in (0, 1]\n",
      "        * Characterizes fading of running costs along horizon\n",
      "\n",
      "    ----------\n",
      "    Attributes\n",
      "    ----------\n",
      "\n",
      "    A, B, C, D : float vectors\n",
      "        * vectors denoting model parameters\n",
      "\n",
      "    my_model : object of type `_model` class\n",
      "\n",
      "    R1, R2 : float vectors\n",
      "        * running cost parameters\n",
      "\n",
      "    u_min, u_max : float vectors\n",
      "        * denoting the min and max control action values\n",
      "\n",
      "    u_buffer : float vector\n",
      "        * buffer of previous controls\n",
      "\n",
      "    y_buffer : float vector\n",
      "        * buffer of previous outputs\n",
      "\n",
      "    ----------\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Osinenko, Pavel, et al. \"Stacked adaptive dynamic programming with unknown system model.\" IFAC-PapersOnLine 50.1 (2017): 4150-4155\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Replace 'None' with the method call\n",
    "\"\"\"\n",
    "\n",
    "### SOLUTION BEGIN\n",
    "p11 = Controller.print_docstring()\n",
    "### SOLUTION END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "t0=0\n",
      "t1=15\n",
      "n_actor=10\n",
      "n_critic=50\n",
      "buffer_size=50\n",
      "ctrl_mode=3\n",
      "critic_mode=1\n",
      "critic_update_time=0.1\n",
      "r_cost_struct=1\n",
      "sample_time=0.2\n",
      "pred_step_size=1\n",
      "estimator_update_time=0.1\n",
      "estimator_buffer_fill=6\n",
      "estimator_buffer_power=2\n",
      "stacked_model_params=0\n",
      "model_order=3\n",
      "gamma=1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Controller.print_init_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading:</font>\n",
    "Run this cell to save your answer. Make sure you defined the necessary variable above to avoid a `NameError` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADING DO NOT MODIFY\n",
    "lab1_answers.record('problem_1-1', p11)\n",
    "\n",
    "### DO NOT MODIFY\n",
    "if hasattr(Controller, 'func_has_been_called'):\n",
    "    setattr(Controller, 'func_has_been_called', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem_1-1': True,\n",
       " 'problem_1-2': {'func_called': True, 'ctrl_mode': 3},\n",
       " 'problem_2-1': 30,\n",
       " 'problem_2-2': {'initial_x': -5,\n",
       "  'initial_y': -5,\n",
       "  'estimator_buffer_power': 6,\n",
       "  'estimator_buffer_fill': 2,\n",
       "  'gamma': 0.95,\n",
       "  'ctrl_mode': 3,\n",
       "  't1': 22,\n",
       "  't_elapsed': 22,\n",
       "  'l2_norm': 0.0041550197105639656},\n",
       " 'problem_2-3': {'t_elapsed': [25, 25], 'winner': 1},\n",
       " 'problem_2-4': {'num_controllers': 3,\n",
       "  'is_visualization': False,\n",
       "  'agent1_t1': 20,\n",
       "  'agent2_t1': 30,\n",
       "  'agent3_t1': 40}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab1_answers.answer_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "md_s1_7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Analysis\n",
    "\n",
    "Examine the docstring carefully. Don't be too hard on yourself - but make sure you have a basic intuition of the meaning of the variables above. We will learn the specifics in detail later (in the next homework assignments).\n",
    "\n",
    "<h2 style=\"color:blue;\">Problem 1.2</h2>\n",
    "\n",
    "#### üéØ Task: In the code cell below, find and specify the default value for the parameter `ctrl_mode` from the `Controller` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Problem_1-2_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "t0=0\n",
      "t1=15\n",
      "n_actor=10\n",
      "n_critic=50\n",
      "buffer_size=50\n",
      "ctrl_mode=3\n",
      "critic_mode=1\n",
      "critic_update_time=0.1\n",
      "r_cost_struct=1\n",
      "sample_time=0.2\n",
      "pred_step_size=1\n",
      "estimator_update_time=0.1\n",
      "estimator_buffer_fill=6\n",
      "estimator_buffer_power=2\n",
      "stacked_model_params=0\n",
      "model_order=3\n",
      "gamma=1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Between the comments below, add your answer to a variable named `ctrl_mode`.\n",
    "\"\"\"\n",
    "\n",
    "### SOLUTION BEGIN\n",
    "p12 = Controller.print_init_params()\n",
    "ctrl_mode = 3\n",
    "### SOLUTION END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading:</font>\n",
    "Run this cell to save your answer. Make sure you defined the necessary variables above to avoid a `NameError` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Problem_1-2_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### GRADING DO NOT MODIFY\n",
    "lab1_answers.record('problem_1-2', {'func_called': p12, 'ctrl_mode': ctrl_mode})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "md_s1_8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Analysis\n",
    "\n",
    "Read the options for `ctrl_mode`. Notice the words 'discretized model'... What does discretized mean ü§î?\n",
    "\n",
    "In RL there are 2 types of state-spaces: *continuous* and *discrete*. A discrete state takes on values from a finite set of values (i.e. discrete states have a **countable** number of possible states).\n",
    "\n",
    "On the other hand, a state is continuous if it takes on values from a continuous function. Continuous states have infinite possible values. \n",
    "\n",
    "Discretization is the process of dividing **(1)** a continuous state-space into discrete (finite) state-space\n",
    "and **(2)** dividing a continuous action-space into discrete action-space in order to formulate the RL learning task in a way that can be numerically approximated (i.e. so that the the optimal value function can be computationally approximated).\n",
    "\n",
    "The concept of discretization also applies to time, especially in the context of the simulations we will see next. Consider that an agent can sample the environment at potentially infinite time intervals. Time is thus discretized into 'time-steps', during which sampling, observing, and stepping is conducted.\n",
    "\n",
    "<h2 style=\"color:blue;\">Exercise 2 - Running simulations</h2>\n",
    "\n",
    "Let's run a simulation in Rcognita. Don't worry, we'll take it in steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Optimal controller (a.k.a. agent) class.\n",
      "\n",
      "    ----------\n",
      "    Parameters\n",
      "    ----------\n",
      "\n",
      "    system : object of type `System` class\n",
      "        object of type System (class)\n",
      "\n",
      "    t0 : int\n",
      "        * Initial value of the controller's internal clock\n",
      "\n",
      "    t1 : int\n",
      "        * End value of controller's internal clock\n",
      "\n",
      "    n_actor : int\n",
      "        Number of prediction steps. n_actor=1 means the controller is purely data-driven and doesn't use prediction.\n",
      "\n",
      "    n_critic : int\n",
      "        Critic stack size. The critic optimizes the temporal error, a.k.a. the value (of state) function. The temporal errors are stacked up using the said buffer.\n",
      "\n",
      "    buffer_size : int\n",
      "        The size of the buffer to store data for model estimation. The bigger the buffer, the more accurate the estimation may be achieved. Using a larger buffer results in better model estimation at the expense of computational cost.\n",
      "\n",
      "    ctrl_mode : int\n",
      "        Modes with online model estimation are experimental\n",
      "        * 0 : manual constant control (only for basic testing)\n",
      "        * -1 : nominal parking controller (for benchmarking optimal controllers)\n",
      "        * 1 : model-predictive control (MPC). Prediction via discretized true model\n",
      "        * 2 : adaptive MPC. Prediction via estimated model\n",
      "        * 3 : RL: Q-learning with n_critic roll-outs of running cost. Prediction via discretized true model\n",
      "        * 4 : RL: Q-learning with n_critic roll-outs of running cost. Prediction via estimated model\n",
      "        * 5 : RL: stacked Q-learning. Prediction via discretized true model\n",
      "        * 6 : RL: stacked Q-learning. Prediction via estimated model\n",
      "\n",
      "        * Modes 1, 3, 5 use model for prediction, passed into class exogenously. This could be, for instance, a true system model\n",
      "        * Modes 2, 4, 6 use an estimated online\n",
      "\n",
      "    critic_mode : int\n",
      "        Choice of the structure of the critic's feature vector\n",
      "        * 1 - Quadratic-linear\n",
      "        * 2 - Quadratic\n",
      "        * 3 - Quadratic, no mixed terms\n",
      "        * 4 - Quadratic, no mixed terms in input and output\n",
      "\n",
      "    critic_update_time : float\n",
      "        * Time between critic updates\n",
      "\n",
      "    r_cost_struct : int\n",
      "        * Choice of the running cost structure. A typical choice is quadratic of the form [y, u].T * R1 [y, u], where R1 is the (usually diagonal) parameter matrix. For different structures, R2 is also used.\n",
      "        * 1 - quadratic chi.T @ R1 @ chi\n",
      "        * 2 - 4th order chi**2.T @ R2 @ chi**2 + chi.T @ R2 @ chi\n",
      "\n",
      "    sample_time : int or float\n",
      "        Controller's sampling time (in seconds). The system itself is continuous as a physical process while the controller is digital.\n",
      "        * the higher the sampling time, the more chattering in the control might occur. It even may lead to instability and failure to park the robot\n",
      "        * smaller sampling times lead to higher computation times\n",
      "        * especially controllers that use the estimated model are sensitive to sampling time, because inaccuracies in estimation lead to problems when propagated over longer periods of time. Experiment with sample_time and try achieve a trade-off between stability and computational performance\n",
      "\n",
      "    pred_step_size : float\n",
      "        * Prediction step size in `J` (in seconds). Is the time between the computation of control inputs and outputs J. Should be a multiple of `sample_time`.\n",
      "\n",
      "    estimator_buffer_fill : int\n",
      "        * Initial phase to fill the estimator's buffer before applying optimal control (in seconds)\n",
      "\n",
      "    estimator_buffer_power : int\n",
      "        * Power of probing noise during an initial phase to fill the estimator's buffer before applying optimal control\n",
      "\n",
      "    estimator_update_time : float\n",
      "        * In seconds, the time between model estimate updates. This constant determines how often the estimated parameters are updated. The more often the model is updated, the higher the computational burden is. On the other hand, more frequent updates help keep the model actual.\n",
      "\n",
      "    stacked_model_params : int\n",
      "        * Estimated model parameters can be stored in stacks and the best among the `stacked_model_params` last ones is picked.\n",
      "        * May improve the prediction quality somewhat\n",
      "\n",
      "    model_order : int\n",
      "        * The order of the state-space estimation model. We are interested in adequate predictions of y under given u's. The higher the model order, the better estimation results may be achieved, but be aware of overfitting.\n",
      "\n",
      "    gamma : float\n",
      "        * Discounting factor\n",
      "        * number in (0, 1]\n",
      "        * Characterizes fading of running costs along horizon\n",
      "\n",
      "    ----------\n",
      "    Attributes\n",
      "    ----------\n",
      "\n",
      "    A, B, C, D : float vectors\n",
      "        * vectors denoting model parameters\n",
      "\n",
      "    my_model : object of type `_model` class\n",
      "\n",
      "    R1, R2 : float vectors\n",
      "        * running cost parameters\n",
      "\n",
      "    u_min, u_max : float vectors\n",
      "        * denoting the min and max control action values\n",
      "\n",
      "    u_buffer : float vector\n",
      "        * buffer of previous controls\n",
      "\n",
      "    y_buffer : float vector\n",
      "        * buffer of previous outputs\n",
      "\n",
      "    ----------\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Osinenko, Pavel, et al. \"Stacked adaptive dynamic programming with unknown system model.\" IFAC-PapersOnLine 50.1 (2017): 4150-4155\n",
      "\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Controller.print_docstring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "code_s1_9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from rcognita import System, NominalController, Controller, Simulation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Step 1: Instantiate the environment\n",
    "sys = System()\n",
    "\n",
    "# Step 2: Instantiate the agent/controller\n",
    "agent = Controller(sys,\n",
    "                    ctrl_mode=5,\n",
    "                    buffer_size=50,\n",
    "                    n_actor=4,\n",
    "                    n_critic=20,\n",
    "                    critic_mode=3,\n",
    "                    t1 = 30,\n",
    "                    estimator_buffer_power=8, \n",
    "                    estimator_buffer_fill=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "md_s1_10",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A quick description of the controller arguments:\n",
    "\n",
    "* ### ctrl_mode\n",
    "    - Stacked Q-learning. Prediction via discretized true model\n",
    "* ### buffer_size\n",
    "    - size of the buffer to store data for model estimation\n",
    "* ### n_actor\n",
    "    - The number of time steps the actor is predicting into the future (from the current time step)\n",
    "* ### n_critic\n",
    "    - The number of temporal errors (from value function) that are used from the buffer\n",
    "    - cannot be larger than buffer_size\n",
    "* ### critic_mode\n",
    "    - Choice of the structure of the critic's feature vector\n",
    "    - 3 is quadratic, no mixed terms\n",
    "* ### estimator_buffer_power\n",
    "    - Power of probing noise during an initial phase to fill the estimator's buffer before applying optimal control\n",
    "* ### t1\n",
    "    * end time\n",
    "* ### estimator_buffer_fill\n",
    "    - The number of seconds of sampling to fill the buffer before action selection\n",
    "    \n",
    "Continuing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "code_s1_11",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: instantiate the nominal controller\n",
    "nominalCtrl = NominalController()\n",
    "\n",
    "# Step 4: instantiate the simulation\n",
    "sim = Simulation(sys, agent, nominalCtrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "md_s1_12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 5: run the simulation\n",
    "<font color=\"red\">‚ùó**Note**:</font> \n",
    "* Do not scroll your notebook while running the simulation to avoid lag\n",
    "* Press 'q' to quit the simulation prematurely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "code_s1_12-5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mikhailgasanov/opt/anaconda3/envs/rcognita/lib/python3.7/tkinter/__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/Users/mikhailgasanov/opt/anaconda3/envs/rcognita/lib/python3.7/tkinter/__init__.py\", line 749, in callit\n",
      "    func(*args)\n",
      "  File \"/Users/mikhailgasanov/opt/anaconda3/envs/rcognita/lib/python3.7/site-packages/matplotlib/backends/_backend_tk.py\", line 253, in idle_draw\n",
      "    self.draw()\n",
      "  File \"/Users/mikhailgasanov/opt/anaconda3/envs/rcognita/lib/python3.7/site-packages/matplotlib/backends/backend_tkagg.py\", line 10, in draw\n",
      "    _backend_tk.blit(self._tkphoto, self.renderer._renderer, (0, 1, 2, 3))\n",
      "  File \"/Users/mikhailgasanov/opt/anaconda3/envs/rcognita/lib/python3.7/site-packages/matplotlib/backends/_backend_tk.py\", line 75, in blit\n",
      "    photoimage.blank()\n",
      "  File \"/Users/mikhailgasanov/opt/anaconda3/envs/rcognita/lib/python3.7/tkinter/__init__.py\", line 3548, in blank\n",
      "    self.tk.call(self.name, 'blank')\n",
      "_tkinter.TclError: invalid command name \"pyimage10\"\n"
     ]
    }
   ],
   "source": [
    "# do not delete\n",
    "#%matplotlib notebook \n",
    "%matplotlib tk\n",
    "\n",
    "# adjust the figure width and height to suit the cell size of your jupyter notebook\n",
    "HTML(sim.run_simulation(fig_width=7, fig_height=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.t_elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sim.run_simulation(fig_width=17, fig_height=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "code_s1_12-6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The simulation should have ran the full duration of 30 seconds. We can confirm this by checking the `t_elapsed` attribute of the `sim` object. \n",
    "\n",
    "<h2 style=\"color:blue;\">Problem 2.1</h2>\n",
    "\n",
    "#### üéØ Task: Print the `t_elapsed` attribute below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.t1 - sim.t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "problem_2-1_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Between the comments below, record the value of sim.t_elapsed into the provided variable\n",
    "\"\"\"\n",
    "\n",
    "### SOLUTION BEGIN\n",
    "sim_t_elapsed = sim.t_elapsed\n",
    "### SOLUTION END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading:</font>\n",
    "Run this cell to save your answer. Make sure you defined the necessary variable above to avoid a `NameError` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "problem_2-1_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### GRADING DO NOT MODIFY\n",
    "lab1_answers.record('problem_2-1', sim_t_elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "md_s1_13",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Side note\n",
    "\n",
    "üîî You can also run Rcognita in a separate window with the commands below. Note that you will need to restart your Jupyter kernel **before** and **after**.\n",
    "\n",
    "Try using different matplotlib backends if one fails.\n",
    "```\n",
    "%matplotlib tk\n",
    "sim.run_simulation(close_plt_on_finish = False)\n",
    "```\n",
    "\n",
    "or \n",
    "\n",
    "\n",
    "```\n",
    "%matplotlib qt\n",
    "sim.run_simulation(close_plt_on_finish = False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "md_s2_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h2 style=\"color:blue;\">Problem 2.2 - create your own simulation</h2>\n",
    "\n",
    "####  üéØ Task: Create a  `Controller` (object) that achieves a L2-norm of < 0.05 in 22 seconds\n",
    "* **Definition**: The L2 norm (aka Euclidean norm) is the distance of the controller ((x,y) coordinates) to (0,0) for a given time-step `t`.\n",
    "* The Euclidean norm is found in the top-right 'Proximity-to-Target' plot during simulation visualization.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "Pass the following parameters to instantiate the `Controller` class. The **bold** parameters are provided for you to plug in. You must determine the <font color=\"orange\">orange</font> parameters.\n",
    "\n",
    "`System` class:\n",
    "* **initial_x = -5**\n",
    "* **initial_y = -5**\n",
    "\n",
    "`Controller` class:\n",
    "* **estimator_buffer_power = 6** \n",
    "* **estimator_buffer_fill = 2**\n",
    "* **gamma = 0.95**\n",
    "* **t1 = 22**\n",
    "* <font color=\"orange\">ctrl_mode</font> (hint: use a discretized model)\n",
    "* <font color=\"orange\">buffer_size</font>\n",
    "* <font color=\"orange\">n_actor</font>\n",
    "* <font color=\"orange\">n_critic</font>\n",
    "* <font color=\"orange\">estimator_update_time</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "t0=0\n",
      "t1=15\n",
      "n_actor=10\n",
      "n_critic=50\n",
      "buffer_size=50\n",
      "ctrl_mode=3\n",
      "critic_mode=1\n",
      "critic_update_time=0.1\n",
      "r_cost_struct=1\n",
      "sample_time=0.2\n",
      "pred_step_size=1\n",
      "estimator_update_time=0.1\n",
      "estimator_buffer_fill=6\n",
      "estimator_buffer_power=2\n",
      "stacked_model_params=0\n",
      "model_order=3\n",
      "gamma=1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Controller.print_init_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor's optimizer failed. Returning default action\n",
      "Actor's optimizer failed. Returning default action\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Only modify code between the comments below. \n",
    "If you need help to determine the parameters, remember to use the functions from Exercise 1.\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from rcognita import System, NominalController, Controller, Simulation\n",
    "from IPython.display import HTML\n",
    "\n",
    "### SOLUTION BEGIN\n",
    "\n",
    "sys = System(initial_x=-5, initial_y=-5)\n",
    "\n",
    "# Step 2: Instantiate the agent/controller\n",
    "agent = Controller(sys,\n",
    "                    ctrl_mode=3,\n",
    "                    buffer_size=100,\n",
    "                    n_actor=30,\n",
    "                    n_critic=40,\n",
    "                    critic_mode=3,\n",
    "                    t1 = 22,\n",
    "                    estimator_update_time = 0.001,\n",
    "                    estimator_buffer_power=6, \n",
    "                    estimator_buffer_fill=2,\n",
    "                    gamma=0.95)\n",
    "\n",
    "### SOLUTION END\n",
    "\n",
    "nominal_ctrl = NominalController()\n",
    "\n",
    "sim2 = Simulation(sys, agent, nominal_ctrl)\n",
    "\n",
    "# Run the simulation\n",
    "#%matplotlib notebook\n",
    "#HTML(sim2.run_simulation(fig_width=7, fig_height=7))\n",
    "sim2.run_simulation(fig_width=7, fig_height=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "problem_2-2_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mikhailgasanov/opt/anaconda3/envs/rcognita/lib/python3.7/tkinter/__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/Users/mikhailgasanov/opt/anaconda3/envs/rcognita/lib/python3.7/tkinter/__init__.py\", line 749, in callit\n",
      "    func(*args)\n",
      "  File \"/Users/mikhailgasanov/opt/anaconda3/envs/rcognita/lib/python3.7/site-packages/matplotlib/backends/_backend_tk.py\", line 253, in idle_draw\n",
      "    self.draw()\n",
      "  File \"/Users/mikhailgasanov/opt/anaconda3/envs/rcognita/lib/python3.7/site-packages/matplotlib/backends/backend_tkagg.py\", line 10, in draw\n",
      "    _backend_tk.blit(self._tkphoto, self.renderer._renderer, (0, 1, 2, 3))\n",
      "  File \"/Users/mikhailgasanov/opt/anaconda3/envs/rcognita/lib/python3.7/site-packages/matplotlib/backends/_backend_tk.py\", line 75, in blit\n",
      "    photoimage.blank()\n",
      "  File \"/Users/mikhailgasanov/opt/anaconda3/envs/rcognita/lib/python3.7/tkinter/__init__.py\", line 3548, in blank\n",
      "    self.tk.call(self.name, 'blank')\n",
      "_tkinter.TclError: invalid command name \"pyimage50\"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Only modify code between the comments below. \n",
    "If you need help to determine the parameters, remember to use the functions from Exercise 1.\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from rcognita import System, NominalController, Controller, Simulation\n",
    "from IPython.display import HTML\n",
    "\n",
    "### SOLUTION BEGIN\n",
    "sys = System(initial_x = -5, initial_y = -5)\n",
    "#sys = System()\n",
    "# agent = Controller(sys,\n",
    "#                     estimator_buffer_power = 6,\n",
    "#                     estimator_buffer_fill = 2,\n",
    "#                     gamma = 0.95,\n",
    "#                     t1 = 22,\n",
    "#                    #\n",
    "#                     ctrl_mode = 3,# (hint: use a discretized model)\n",
    "#                     buffer_size = 100,\n",
    "#                     n_actor = 30,\n",
    "#                     n_critic = 40,\n",
    "#                     estimator_update_time = 0.01)\n",
    "\n",
    "###MISHA SOLUTION \n",
    "\n",
    "agent = Controller(sys,\n",
    "                    ctrl_mode=3,\n",
    "                    buffer_size=70,\n",
    "                    n_actor=25,\n",
    "                    n_critic=35,\n",
    "                    critic_mode=3,\n",
    "                    t1 = 22,\n",
    "                    estimator_update_time = 0.001,\n",
    "                    estimator_buffer_power=6, \n",
    "                    estimator_buffer_fill=2,\n",
    "                    gamma=0.95)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### SOLUTION END\n",
    "\n",
    "nominal_ctrl = NominalController()\n",
    "\n",
    "sim2 = Simulation(sys, agent, nominal_ctrl)\n",
    "\n",
    "# Run the simulation\n",
    "#%matplotlib tk\n",
    "#matplotlib notebook\n",
    "#HTML(sim2.run_simulation(fig_width=7, fig_height=7))\n",
    "sim2.run_simulation(fig_width=7, fig_height=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0041550197105639656"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim2.l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading:</font>\n",
    "Run this cell to save your answer. Make sure you finished the simulation above to avoid an error below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "problem_2-2_test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### GRADING DO NOT MODIFY\n",
    "lab1_answers.record('problem_2-2', {'initial_x': sys.initial_x, \n",
    "                                    'initial_y': sys.initial_y, \n",
    "                                    'estimator_buffer_power': agent.estimator_buffer_power, \n",
    "                                    'estimator_buffer_fill': agent.estimator_buffer_fill, \n",
    "                                    'gamma': agent.gamma, \n",
    "                                    'ctrl_mode': agent.ctrl_mode, \n",
    "                                    't1': agent.t1,\n",
    "                                    't_elapsed': sim2.t_elapsed,\n",
    "                                    'l2_norm': sim2.l2_norm})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "md_s2_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h2 style=\"color:blue;\">Problem 2.3 - Sample Time and Prediction Step Size</h2>\n",
    "\n",
    "Let's think about 2 important parameters from the `Controller` class:\n",
    "* `sample_time`\n",
    "* `pred_step_size`\n",
    "\n",
    "Sample time refers to the controller's sampling time: **how often in seconds the controller samples the environment**. This \"sampling\" of the environment simply means that the controller observes the state of the environment to decide what action to take next.\n",
    "\n",
    "Prediction step size refers to the time between performing control inputs (i.e taking actions) and receiving outputs from the environment. Thus, this period **implicitly refers to how often the controller takes actions**.\n",
    "\n",
    "Analyze: how are these two variables related?\n",
    "* Consider *sample time* < *prediction step size* (the agent samples the environment **more frequently** than taking actions)\n",
    "* And consider if *sample time* > *prediction step size* (the agent samples the environment **less frequently** than taking actions)\n",
    "* Note that: $\\text{frequency} = {{1}\\over{period}}$\n",
    "\n",
    "\n",
    "Let's consider this scenario below:\n",
    "* Agent 1 has a sample time of 0.3 seconds and prediction step size of 0.6 seconds\n",
    "* Agent 2 has a sample time of 0.6 seconds and prediction step size of 0.3 seconds\n",
    "\n",
    "#### üéØ  Task: Run the multi-controller simulation prepared for you below to analyze this scenario. \n",
    "\n",
    "üîî **Note:**\n",
    "- Each agent performs 2 runs\n",
    "- Each agent has a maximum sample time of 25 time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "code_s2_3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Cannot change to a different GUI toolkit: notebook. Using tk instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor's optimizer failed. Returning default action\n",
      "Actor's optimizer failed. Returning default action\n",
      "Actor's optimizer failed. Returning default action\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mikhailgasanov/opt/anaconda3/envs/rcognita/lib/python3.7/tkinter/__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/Users/mikhailgasanov/opt/anaconda3/envs/rcognita/lib/python3.7/tkinter/__init__.py\", line 749, in callit\n",
      "    func(*args)\n",
      "  File \"/Users/mikhailgasanov/opt/anaconda3/envs/rcognita/lib/python3.7/site-packages/matplotlib/backends/_backend_tk.py\", line 253, in idle_draw\n",
      "    self.draw()\n",
      "  File \"/Users/mikhailgasanov/opt/anaconda3/envs/rcognita/lib/python3.7/site-packages/matplotlib/backends/backend_tkagg.py\", line 10, in draw\n",
      "    _backend_tk.blit(self._tkphoto, self.renderer._renderer, (0, 1, 2, 3))\n",
      "  File \"/Users/mikhailgasanov/opt/anaconda3/envs/rcognita/lib/python3.7/site-packages/matplotlib/backends/_backend_tk.py\", line 75, in blit\n",
      "    photoimage.blank()\n",
      "  File \"/Users/mikhailgasanov/opt/anaconda3/envs/rcognita/lib/python3.7/tkinter/__init__.py\", line 3548, in blank\n",
      "    self.tk.call(self.name, 'blank')\n",
      "_tkinter.TclError: invalid command name \"pyimage60\"\n"
     ]
    }
   ],
   "source": [
    "from rcognita import System, NominalController, Controller, Simulation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# create system\n",
    "sys = System(initial_x=7, initial_y=7) # creates bot #1\n",
    "\n",
    "# add a bot to coordinate (-7, -7)\n",
    "sys.add_bots(-7,-7) # creates bot #2\n",
    "\n",
    "# create two nominal controllers to serve as baseline for each controller\n",
    "nominal_ctrl1 = NominalController()\n",
    "nominal_ctrl2 = NominalController()\n",
    "\n",
    "agent1 = Controller(sys,\n",
    "                    sample_time=0.3,\n",
    "                    pred_step_size=0.6,\n",
    "                    critic_mode=3,\n",
    "                    ctrl_mode=3,\n",
    "                    buffer_size=20,\n",
    "                    n_actor=10,\n",
    "                    n_critic=10,\n",
    "                    t1=25,\n",
    "                    estimator_update_time=0.3)\n",
    "\n",
    "agent2 = Controller(sys,\n",
    "                    sample_time=0.6,\n",
    "                    pred_step_size=0.3,\n",
    "                    critic_mode=3,\n",
    "                    ctrl_mode=3,\n",
    "                    buffer_size=20,\n",
    "                    n_actor=10,\n",
    "                    n_critic=10,\n",
    "                    t1=25,\n",
    "                    estimator_update_time=0.3)\n",
    "\n",
    "sim3 = Simulation(sys, [agent1, agent2], [nominal_ctrl1, nominal_ctrl2])\n",
    "\n",
    "# Run the simulation\n",
    "%matplotlib notebook\n",
    "HTML(sim3.run_simulation(n_runs=2, fig_width=7, fig_height=7, show_annotations=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "md_s2_4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Analysis\n",
    "\n",
    "After viewing the simulation, which agent do you think was more successful in minimizing the objective function (reducing the running cost to arrive at its target coordinates)? If you **correctly** followed the instructions for the parameters above, the result should be obvious. In the code cell below, write your answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "problem_2-3_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define which agent was more successful in the variable `winner` below\n",
    "by specifying `winner = 1` for agent 1 or `winner = 2` for agent 2.\n",
    "\"\"\"\n",
    "\n",
    "### SOLUTION BEGIN\n",
    "winner = 1\n",
    "### SOLUTION END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading:</font>\n",
    "Run this cell to save your answer. Make sure you defined the necessary variables above to avoid a `NameError` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "problem_2-3_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### GRADING DO NOT MODIFY\n",
    "lab1_answers.record('problem_2-3', {'t_elapsed': sim3.t_elapsed, 'winner': winner})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "md_s2_5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can find out for sure which agent was a more effective learner by printing some statistics from the simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "code_s2_6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runs for each controller: 2\n",
      "Statistics for controller 1:\n",
      "            - Mean of running cost: 183.63\n",
      "            - Variance of running cost: 106017.03\n",
      "            - Mean of velocity: -0.45\n",
      "            - Variance of velocity: 0.61\n",
      "            - Variance of alpha: 0.34\n",
      "            - Final L2-norm: 0.33\n",
      "                \n",
      "Statistics for controller 2:\n",
      "            - Mean of running cost: 263.29\n",
      "            - Variance of running cost: 93885.98\n",
      "            - Mean of velocity: 1.11\n",
      "            - Variance of velocity: 1.12\n",
      "            - Variance of alpha: 2.5\n",
      "            - Final L2-norm: 3.29\n",
      "                \n"
     ]
    }
   ],
   "source": [
    "sim3.print_simu_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "md_s2_7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Examine and compare the statistics above for each controller. \n",
    "\n",
    "The most important statistics above are the mean of running cost $\\mu_{cost}$, variance of running cost $\\sigma^2_{cost}$, and variance of velocity $\\sigma^2_{velocity}$\n",
    "\n",
    "If you correctly implemented the instructions for the simulation, then three observations can be made:\n",
    "\n",
    "1. By comparing $\\mu_{cost}$ for both controllers, we see that *on average* controller 1 was better at selecting actions for observations from the environment. Controller 1 learned a better model of the environment for maximizing value (and thus reducing cost) over the same time horizon than Controller 2.\n",
    "\n",
    "2. Controller 1 has a lower $\\sigma^2_{velocity}$ than controller 2. This means controller 1 was *more stable* and less eratic in its charted path, and thus was more sensitive in the magnitude of its selected actions. It charted a slower but more sure path towards the target coordinates by sampling the environment more frequently and taking actions less frequently than controller 2.\n",
    "\n",
    "3. The L2-norm of controller 1 is lower, indicating a better position of the robot in relation to its target thus a more accurate learned policy.\n",
    "\n",
    "Also, notice that a reduced sampling time period leads to much quicker episodes (i.e. for controller 2).\n",
    "\n",
    "In conclusion, we can say that controller 1 was a \"sure-and-steady\" learner; one that learned a better policy to maximize value than controller 2.\n",
    "\n",
    "### Something to think about\n",
    "\n",
    "Going forward, as you work with different environments in reinforcement learning -- try to think about the implications of the frequency of sampling the environment versus the frequency of taking actions. There is a balance to be made between these two parameters and this balance will differ depending on the environment and the challenges of that environment.\n",
    "\n",
    "### Lastly\n",
    "\n",
    "You should also know that it's possible to run Rcognita without visualizations. This option is much faster computationally.\n",
    "\n",
    "<h2 style=\"color:blue;\">Problem 2.4 - Running Rcognita without the visualization</h2>\n",
    "\n",
    "#### üéØ  Task: Run a simulation with 3 agents. \n",
    "\n",
    "üîî **Note**:\n",
    "* Notice how `add_bots()` is utilized to create additional controllers\n",
    "* The `System()` class by default creates 1 controller.\n",
    "\n",
    "**Requirements:**\n",
    "* Create 3 controller objects\n",
    "* Create 3 nominal controller objects\n",
    "* Controller 1 should have `t1 = 20`\n",
    "* Controller 2 should have `t1 = 30`\n",
    "* Controller 3 should have `t1 = 40`\n",
    "* For Simulation class, use `n_runs=2`\n",
    "* For Simulation class, use `print_summary_stats=True` \n",
    "* For Simulation class, use `print_statistics_at_step=False`\n",
    "* For the remaining controller parameters (for each controller), free free to copy the same parameters used in problem 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "problem_2-4_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Running for controller 1\n",
      "... Running for controller 2\n",
      "... Running for controller 3\n",
      "Total runs for each controller: 2\n",
      "Statistics for controller 1:\n",
      "            - Mean of running cost: 99.85\n",
      "            - Variance of running cost: 29249.08\n",
      "            - Mean of velocity: -0.31\n",
      "            - Variance of velocity: 0.3\n",
      "            - Variance of alpha: 0.24\n",
      "            - Final L2-norm: 0.02\n",
      "                \n",
      "Statistics for controller 2:\n",
      "            - Mean of running cost: 65.01\n",
      "            - Variance of running cost: 21728.54\n",
      "            - Mean of velocity: 0.21\n",
      "            - Variance of velocity: 0.27\n",
      "            - Variance of alpha: 0.22\n",
      "            - Final L2-norm: 0.1\n",
      "                \n",
      "Statistics for controller 3:\n",
      "            - Mean of running cost: 153.94\n",
      "            - Variance of running cost: 73011.67\n",
      "            - Mean of velocity: -0.65\n",
      "            - Variance of velocity: 0.68\n",
      "            - Variance of alpha: 2.02\n",
      "            - Final L2-norm: 0.02\n",
      "                \n"
     ]
    }
   ],
   "source": [
    "from rcognita import System, NominalController, Controller, Simulation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# environment\n",
    "sys = System() # already creates bot #1\n",
    "sys.add_bots(-5,-5) # creates bot #2\n",
    "sys.add_bots(-7,7) # creates bot #3\n",
    "\n",
    "### SOLUTION BEGIN\n",
    "\n",
    "nominal_ctrl1 = NominalController()\n",
    "nominal_ctrl2 = NominalController()\n",
    "nominal_ctrl3 = NominalController()\n",
    "\n",
    "agent1 = Controller(sys,\n",
    "                    sample_time=0.3,\n",
    "                    pred_step_size=0.6,\n",
    "                    critic_mode=3,\n",
    "                    ctrl_mode=3,\n",
    "                    buffer_size=20,\n",
    "                    n_actor=10,\n",
    "                    n_critic=10,\n",
    "                    t1=20,\n",
    "                    estimator_update_time=0.3)\n",
    "\n",
    "agent2 = Controller(sys,\n",
    "                    sample_time=0.3,\n",
    "                    pred_step_size=0.6,\n",
    "                    critic_mode=3,\n",
    "                    ctrl_mode=3,\n",
    "                    buffer_size=20,\n",
    "                    n_actor=10,\n",
    "                    n_critic=10,\n",
    "                    t1=30,\n",
    "                    estimator_update_time=0.3)\n",
    "\n",
    "agent3 = Controller(sys,\n",
    "                    sample_time=0.3,\n",
    "                    pred_step_size=0.6,\n",
    "                    critic_mode=3,\n",
    "                    ctrl_mode=3,\n",
    "                    buffer_size=20,\n",
    "                    n_actor=10,\n",
    "                    n_critic=10,\n",
    "                    t1=40,\n",
    "                    estimator_update_time=0.3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### SOLUTION END\n",
    "\n",
    "sim4 = Simulation(sys, \n",
    "                  [agent1, agent2, agent3], \n",
    "                  [nominal_ctrl1, nominal_ctrl2, nominal_ctrl3])\n",
    "\n",
    "sim4.run_simulation(n_runs=2, \n",
    "                    is_visualization=False,\n",
    "                    print_summary_stats=True, \n",
    "                    print_statistics_at_step=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading:</font>\n",
    "Run this cell to save your answer. Make sure you performed the simulation above to avoid a `NameError` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem_1-1': True,\n",
       " 'problem_1-2': {'func_called': True, 'ctrl_mode': 3},\n",
       " 'problem_2-1': 30,\n",
       " 'problem_2-2': {'initial_x': -5,\n",
       "  'initial_y': -5,\n",
       "  'estimator_buffer_power': 6,\n",
       "  'estimator_buffer_fill': 2,\n",
       "  'gamma': 0.95,\n",
       "  'ctrl_mode': 3,\n",
       "  't1': 22,\n",
       "  't_elapsed': 22,\n",
       "  'l2_norm': 0.0041550197105639656},\n",
       " 'problem_2-3': {'t_elapsed': [25, 25], 'winner': 1},\n",
       " 'problem_2-4': {'num_controllers': 3,\n",
       "  'is_visualization': False,\n",
       "  'agent1_t1': 20,\n",
       "  'agent2_t1': 30,\n",
       "  'agent3_t1': 40}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab1_answers.answer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "problem-2-4_test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'problem_1-1': <bound method check_call.<locals>.wrapper of <class 'rcognita.rlframe.Controller'>>, 'problem_1-2': {'func_called': True, 'ctrl_mode': 3}, 'problem_2-1': 30, 'problem_2-2': {'initial_x': -5, 'initial_y': -5, 'estimator_buffer_power': 6, 'estimator_buffer_fill': 2, 'gamma': 0.95, 'ctrl_mode': 3, 't1': 22, 't_elapsed': 22, 'l2_norm': 0.0041550197105639656}, 'problem_2-3': {'t_elapsed': [25, 25], 'winner': 1}, 'problem_2-4': {'num_controllers': 3, 'is_visualization': False, 'agent1_t1': 20, 'agent2_t1': 30, 'agent3_t1': 40}}\n"
     ]
    }
   ],
   "source": [
    "### GRADING DO NOT MODIFY\n",
    "lab1_answers.record('problem_2-4', {'num_controllers': sim4.num_controllers, \n",
    "                                    'is_visualization': sim4.is_visualization,\n",
    "                                   'agent1_t1': agent1.t1,\n",
    "                                   'agent2_t1': agent2.t1,\n",
    "                                   'agent3_t1': agent3.t1,})\n",
    "\n",
    "lab1_answers.print_answers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading: Submit your answers</font>\n",
    "Enter your first and last name in the cell below and then run it to save your answers for this lab to a JSON file. The file is saved to the same directory as this notebook. After the file is created, upload the JSON file to the assignment page on Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type method is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-9eb4fddd128f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlast_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Illarionova\"\u001b[0m \u001b[0;31m# Use proper capitalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlab1_answers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_to_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massignment_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/rcognita/lib/python3.7/site-packages/rcognita/grading_utilities.py\u001b[0m in \u001b[0;36msave_to_json\u001b[0;34m(self, assignment_name, first_name, last_name)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_n\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msubmitted_answers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manswer_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmitted_answers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/rcognita/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/rcognita/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/rcognita/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/rcognita/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/rcognita/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type method is not JSON serializable"
     ]
    }
   ],
   "source": [
    "assignment_name = \"lab_1\"\n",
    "first_name = \"Svetlana\" # Use proper capitalization\n",
    "last_name = \"Illarionova\" # Use proper capitalization\n",
    "\n",
    "lab1_answers.save_to_json(assignment_name, first_name, last_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóFailure to properly submit this JSON file by the due date will result in a failed grade - so please reach out **ahead of the deadline** if you have questions or problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "footnote",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Questions?\n",
    "\n",
    "Reach out to your instructors on Piazza (you can find it in Canvas on the left-most menu).\n",
    "\n",
    "## Sources\n",
    "\n",
    "***\n",
    "\n",
    "<sup>[1]</sup> Barnab√°s P√≥czos, Carnegie Mellon, Introduction To Machine Learning: Reinforcement Learning (Course).\n",
    "\n",
    "<sup>[2]</sup> Levine, S., Finn, C., Darrell, T., & Abbeel, P. (2016). End-to-End Training of Deep Visuomotor Policies. J. Mach. Learn. Res., 17, 39:1-39:40.\n",
    "\n",
    "<sup>[3]</sup> Busoniu, L., Babu≈°ka, R., Schutter, B.D., & Ernst, D. (2010). Reinforcement Learning and Dynamic Programming Using Function Approximators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "copyright",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "***\n",
    "\n",
    "<h6 style=\"color:#A3A3A3; text-align: center\">Not to be distributed - Copyright AIDA Lab Skoltech</h6>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
