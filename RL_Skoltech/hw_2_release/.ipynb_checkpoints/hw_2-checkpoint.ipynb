{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"skoltech_logo.png\" alt=\"Skoltech\" width=80% height=60% style=\"padding-right:80px;\"/>\n",
    "<h1 style=\"color:#333333; text-align:center; line-height: 0;\">Reinforcement Learning</h1>\n",
    "<h5 style=\"color:#333333; text-align:center;\">Course MA030422</h5>\n",
    "\n",
    "<h2 style=\"color:#A7BD3F;\">Homework 2</h2>\n",
    "\n",
    "***\n",
    "\n",
    "### Intro\n",
    "\n",
    "#### First, a recap of homework 1\n",
    "\n",
    "In homework 1 we applied value iteration and policy iteration to an environment with discrete state and action spaces (aka a finite MDP). If you recall, the environment we used (FrozenLake 4x4) had 16 states and 4 actions. \n",
    "\n",
    "#### The algorithms from hw 1 have several characteristics:\n",
    "* The tabular representations of MDP variables had to be stored in memory:\n",
    "    * In the case of VI, the values of states, transitions, and rewards were stored in memory and referenced when calculating the best action to take\n",
    "    * In the case of PI, policy was additionally stored in memory.\n",
    "* Neither value nor policy functions were represented mathematically/analytically\n",
    "* Both value iteration (VI) and policy iteration (PI) were *repeatedly and reliably* able to solve the environment (when `is_slippery==False`)\n",
    "\n",
    "#### The problem with *Exact Value Iteration* and *Exact Policy Iteration*\n",
    "\n",
    "The problem with VI and PI from homework 1 is that when MDP variables (states, actions, transitions, or rewards) \"have a very large or infinite number of possible values (e.g., when they are continuous)\", then Tabular/stored \"representations are no longer possible, and value functions and policies need to be represented approximately\" <sup>[2]</sup>.\n",
    "\n",
    "Let's examine this issue in more specific terms. Examine the following VI algorithm <sup>[1]</sup>:\n",
    "\n",
    "<img src=\"vi.png\" width=\"70%\" height=\"70%\" />\n",
    "\n",
    "This algorithm is nearly identical to VI from hw 1. What problems/deficiencies can you identify in this algorithm if we had to scale it by 30 orders of magnitude? \n",
    "\n",
    "At least 5 problems/deficiencies come to mind with regard to scale <sup>[1]</sup>:\n",
    "1. All of the states are iterated over (line 3)\n",
    "2. Value of state $V(s)$ is stored for every state (line 5)\n",
    "3. Policy $\\pi$ is stored for every state (line 6)\n",
    "4. The expected return over future states ($s^\\prime$) is calculated recursively for all $s^\\prime$ (lines 5 and 6)\n",
    "5. Maximization is performed over all possible actions (lines 5 and 6)\n",
    "\n",
    "When designing VI and PI algorithms for **continuous** state and/or action spaces, all of these problems can be addressed with **approximation**, which, when applied to the DP methods (such as VI and PI) is known as **Approximate Dynamic Programming**.\n",
    "\n",
    "### Goal of this homework\n",
    "\n",
    "The purpose of this assignment is to learn about and practice applying approximation methods to continuous space environments, specifically ENDI from Rcognita.\n",
    "\n",
    "### Components\n",
    "\n",
    "* **Section 1**: Concept Review\n",
    "* **Section 2**: Approximate Dynamic Programming\n",
    "    * Exercise 1 - Linear Function Approximation\n",
    "        * Problem 1.1 - 15 points\n",
    "\n",
    "Total points: 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\">Imports and Autograder</h2>\n",
    "\n",
    "***\n",
    "Take care of imports early on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rcognita import EndiSystem, EndiControllerBase, Simulation, AnswerTracker\n",
    "from IPython.display import HTML, clear_output\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading</font>\n",
    "Run this cell to track your answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADING DO NOT MODIFY\n",
    "hw2_answers = AnswerTracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\">Section 1 - Concept Review</h2>\n",
    "\n",
    "***\n",
    "\n",
    "The following concepts will help you in completing this assignment and furthering your understanding of common taxonomy in the field of RL.\n",
    "\n",
    "### Dynamic Programming vs Reinforcement Learning\n",
    "\n",
    "Dynamic programming refers to a class of optimization methods that solve problems through recursion and aggregation, or more formally by \"by combining solutions from their subproblems\" <sup>[1]</sup>.\n",
    "<blockquote><b>DP algorithms</b> require a model of the MDP, including the transition dynamics\n",
    "and the reward function, to find an optimal policy (Bertsekas, 2007; Powell, 2007).\n",
    "Usually, they do not require an analytical expression of the\n",
    "dynamics. Instead, given a state and an action, the model is only required to generate a next state and the corresponding reward. Constructing such a generative model is often easier than deriving an analytical expression of the dynamics, especially when the dynamics are stochastic.<sup>[2]</sup></blockquote>\n",
    "\n",
    "In contrast, in reinforcement learning algorithms, the underlying model of the environment is unknown:\n",
    "<blockquote><b>RL algorithms</b> are model-free (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998), which makes them useful when a model is difficult or costly to construct. RL algorithms use data obtained from the process, in the form of a set of samples, a set of process trajectories, or a single trajectory. So, RL can be seen as model-free, sample- based or trajectory-based DP, and DP can be seen as model-based RL. While DP algorithms can use the model to obtain any number of sample transitions from any state-action pair, RL algorithms must work with the limited data that can be obtained from the process ‚Äì a greater challenge. Note that some RL algorithms build a model from the data; we call these algorithms ‚Äúmodel-learning.‚Äù<sup>[2]</sup></blockquote>\n",
    "\n",
    "### Offline vs Online\n",
    "\n",
    "From [2]:\n",
    "> * Offline RL methods are applicable if data can be obtained in advance. \n",
    "> * Online RL algorithms learn a solution by interacting with the system, and can therefore be applied even when data is not available in advance. For instance, intelligent agents are often placed in environments that are not fully known beforehand, which makes it impossible to obtain data in advance.\n",
    "\n",
    "### Deterministic vs Stochastic\n",
    "\n",
    "\n",
    "* **Deterministic**\n",
    "    * > A deterministic MDP is defined by the state space $X$ of the process, the action space $U$ of the controller, the transition function $f$ of the process (which describes how the state changes as a result of control actions), and the reward function $\\rho$ (which evaluates the immediate control performance). <sup>[2]</sup>\n",
    "* **Stochastic**\n",
    "    * > In a stochastic MDP, the next state is not deterministically given by the current state and action. Instead, the next state is a random variable, and the current state and action give the probability density of this random variable. <sup>[2]</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\">Section 2 - Approximate Dynamic Programming</h2>\n",
    "\n",
    "***\n",
    "\n",
    "From our previous lectures, let us recall the Q-function:\n",
    "\n",
    "$$Q^{\\pi}(s, a)=E_{\\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} r_{t} \\mid s_{0}=s, a_{0}=a\\right]$$\n",
    "\n",
    "... And in expanded form (for finite state and action spaces):\n",
    "\n",
    "$$Q^\\pi{(s,a)} = \\sum_{s^\\prime\\in\\text{S}} P{(s^\\prime|s,a)} \\left[R(s,a,s^\\prime) + \\gamma \\sum_{a^\\prime\\in\\text{A}} \\pi{(a^\\prime|s^\\prime)} Q^\\pi{(s^\\prime,a^\\prime)}\\right]$$\n",
    "\n",
    "\n",
    "... Which can also be represented in terms of state-value:\n",
    "\n",
    "$$Q^{\\pi}(s, a)=\\sum_{s^{\\prime} \\in \\mathcal{S}} P\\left(s^{\\prime} \\mid s, a\\right) \\left[R\\left(s, a, s^{\\prime}\\right)+\\gamma V^{\\pi}\\left(s^{\\prime}\\right)\\right]$$\n",
    "\n",
    "This Q-function is a polynomial of $|S|$ (cardinality), which is projected recursively over a trajectory that, even in the case of a substantially large **finite** state-space, would be computationally infeasible to solve from an iteration-time perspective (as well as being too costly from a memory perspective).\n",
    "\n",
    "The question is, is there an **approximate representation** of $Q(s,a)$ that has a sufficiently small number of parameters (much less than $|S|$)? \n",
    "\n",
    "A simple, linear parametric approximation that works well in practice is the following:\n",
    "\n",
    "$$Q^{\\pi}(s, a)=\\phi(s, a)^{\\top} \\boldsymbol{\\theta}$$\n",
    "\n",
    "Where:\n",
    "* $\\phi(s, a) \\in \\mathbb{R}^{n}$\n",
    "* $\\boldsymbol{\\theta} \\in \\mathbb{R}^{n}$\n",
    "\n",
    "A word about notation:\n",
    "* $\\phi$ is the feature **function**: $\\phi : S \\times A \\rightarrow \\mathbb{R}^{n}$\n",
    "    * \"$\\phi$ maps each state-action pair to a vector of feature values\" <sup>[1]</sup>\n",
    "* $\\phi(s, a)$ is the feature **vector** of $s$ and $a$\n",
    "* $\\phi_i{(s, a)}$ denotes an feature **value** (aka simply feature) of the feature vector for state $s$ and action $a$\n",
    "* $\\theta$ is the weight vector which denotes the contribution of each feature to Q\n",
    "\n",
    "Note:\n",
    "* $\\phi$ can also take only $s$ as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Exercise 1 - Linear Function Approximation</font>\n",
    "\n",
    "To learn and understand value-function **approximation** (specifically Q-function approximation), we will be implementing an algorithm in Rcognita entitled **Trajectory Based Value Iteration Least Squares** (TBVILS) <sup>[1]</sup>:\n",
    "\n",
    "<img src=\"tbvils.png\" width=70% height=70% />\n",
    "\n",
    "### How does it work\n",
    "\n",
    "#### üí° Line 6\n",
    "\n",
    "In line 6, the buffers (of size $\\beta$) are updated with the current state and action.\n",
    "\n",
    "#### üí° Line 7\n",
    "\n",
    "On line 7 we update the weights of Q-function. This is done by minimizing the cost function known as temporal error (aka delta, $\\delta$) with respect to parameters $\\theta$:\n",
    "\n",
    "$$\\delta = (Q^{+}(s, a) - Q(s, a))^{2}$$\n",
    "\n",
    "Where (‚ùóin the case of VI):\n",
    "\n",
    "* **Approximate Q**: $\\hspace{1em} Q(s, a) = W_{j+1}^{T} \\phi\\left(x_{k}\\right)$</font>\n",
    "* **Target Q**: $\\hspace{1em} Q^{+}(s, a) = r\\left(x_{k}, h_{j}\\left(x_{k}\\right)\\right)+W_{j}^{T} \\gamma \\phi\\left(x_{k+1}\\right)$</font>\n",
    "\n",
    "Pay close attention to the differences in $W$ and $x$ above.\n",
    "\n",
    "Thus, for value iteration, for a given time-step, temporal error has the following form <sup>[3]</sup>:\n",
    "\n",
    "$$e_{k}= \\big(r\\left(x_{k}, h_{j}\\left(x_{k}\\right)\\right)+W_{j}^{T} \\gamma \\phi\\left(x_{k+1}\\right) - W_{j+1}^{T} \\phi\\left(x_{k}\\right)\\big)^2$$\n",
    "\n",
    "The task of optimization is to minimize $e_k$ to 0 by adjusting the weights $W$:\n",
    "\n",
    "$$0 = \\big(r\\left(x_{k}, h_{j}\\left(x_{k}\\right)\\right)+W_{j}^{T} \\gamma \\phi\\left(x_{k+1}\\right) - W_{j+1}^{T} \\phi\\left(x_{k}\\right)\\big)^2$$\n",
    "\n",
    "**Side-note**:\n",
    "* In policy iteration, the temporal difference equation looks differently:\n",
    "$$e_{k}= \\big(r\\left(x_{k}, h_{j}\\left(x_{k}\\right)\\right)+W_{j}^{T} \\gamma \\phi\\left(x_{k}\\right) - W_{j}^{T} \\phi\\left(x_{k+1}\\right)\\big)^2$$\n",
    "\n",
    "#### üí° Line 8:\n",
    "\n",
    "This phase is very similar to that of Q-function update, except we are minimizing the $V(s)$ function below with respect to policy weights $\\pi_{W}$.\n",
    "\n",
    "During the policy update phase - the `policy` method in the code performs the following tasks:\n",
    "1. It takes the current state $s$ and initial action $a$ and generates a trajectory of predicted states over the horizon length.\n",
    "2. It then calculates the **sum of discounted rewards** over this trajectory:\n",
    "\n",
    "$$V_{h}\\left(x_{k}\\right)=\\sum_{i=k}^{\\infty} \\gamma^{i-k} r\\left(x_{i}, u_{i}\\right) $$\n",
    "3. Steps 1 and 2 are repeated until cost from step 2 is **minimized** (see next section)\n",
    "\n",
    "The minimization function that we use is scipy's `minimize`, which is responsible for executing step 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before you get started: notation details\n",
    "\n",
    "Rcognita is designed to be used as a framework for DP, RL, *and* control systems. Therefore, we use a variety of notation that is common to these areas.\n",
    "\n",
    "For example, instead of the following:\n",
    "\n",
    "$$Q^{\\pi}(s, a)=\\phi(s, a)^{\\top} \\boldsymbol{\\theta}$$\n",
    "\n",
    "In our code you will see the Q-function represented as <sup>[3]</sup>:\n",
    "\n",
    "$$Q_{h}(x, u)=W^{T} \\phi(x, u)$$\n",
    "\n",
    "Where <sup>[3]</sup>:\n",
    "* $h$ is a **policy** (aka $\\pi$)\n",
    "* $x$ is the **state**\n",
    "* $y$ is the **output** of a system (which is the same thing as $x$ in our code)\n",
    "* $u$ is the **control** (aka action)\n",
    "* $W$ is the weight vector (aka $\\theta$)\n",
    "\n",
    "#### Other related notation and the objective function :\n",
    "\n",
    "* $\\dot{x} = f(x,u)$ - the system dynamics (change in system's state)\n",
    "* $x_k \\in R^n$ - state vector (where $k$ is a time step)\n",
    "* $u_k = h(x_k)$ - control function (a.k.a. policy function)\n",
    "    * where $u_k \\in R^m$\n",
    "* $r(x_k, u_k)$ - utility function, aka instantaneous cost or reward function\n",
    "\n",
    "And finally:\n",
    "\n",
    "$$V_{h}\\left(x_{k}\\right)=\\sum_{i=k}^{\\infty} \\gamma^{i-k} r\\left(x_{i}, u_{i}\\right)$$\n",
    "\n",
    "While in RL this is known as the *state-value function* with respect to a policy $h$, in dynamical systems and DP, \"this is known as the **cost-to-go** and is a sum of discounted future costs from\n",
    "the current time k into the infinite horizon future\" <sup>[3]</sup>. Other names for $V(s)$ include **cost** and **value**.\n",
    "\n",
    "#### ‚ùó In the context of control theory, dynamical systems, and this assignment -- <font color=\"red\">the value function is minimized, not maximized</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Problem 1</font>\n",
    "\n",
    "Let's apply the TBVILS algorithm to the ENDI environment from Rcognita. Here is the algorithm again for convenience <sup>[1]</sup>:\n",
    "\n",
    "<img src=\"tbvils.png\" width=70% height=70% />\n",
    "\n",
    "### Variables in Rcognita\n",
    "\n",
    "Recall that in the ENDI environment of Rcognita:\n",
    "The environment has the following 5 characteristics:\n",
    "* **x ($x_c$)** = x-coordinate (m)\n",
    "* **y ($y_c$)** = y-coordinate (m)\n",
    "* **alpha ($\\alpha$)** = turning angle (rad)\n",
    "* **upsilon ($\\upsilon$)** = velocity (m/s)\n",
    "* **omega ($\\omega$)** = revolution speed (rad/s) (aka turning speed)\n",
    "\n",
    "For a given time-step, $t$:\n",
    "* Action or control input is given by: $\\hspace{3mm}a_t = (F, M)$\n",
    "* Environment is given by: $\\hspace{3mm}s_t = (x_c, y_c, \\alpha, \\upsilon, \\omega)$\n",
    "\n",
    "Thus in the ENDI environment, $\\phi(s,a)$ would be:\n",
    "* $\\phi(s_t, a_t) = (x_c, y_c, \\alpha, \\upsilon, \\omega, F, M)$\n",
    "\n",
    "Usually $\\phi$ takes a quadratic form, in which $\\phi : (S \\times A)^2 \\rightarrow \\mathbb{R}^{n}$\n",
    "\n",
    "**Note**: note that $\\phi$ can also take *only* state (in the case of state-value function).\n",
    "\n",
    "#### üéØ Task: implement the TBVILS algorithm in the code below by filling in the lines specified by comments\n",
    "Remember the key functions from above and implement them:\n",
    "\n",
    "Hints:\n",
    "* Work method by method, filling in the code where the comments specify.\n",
    "* When working on the `value_function`, note the:\n",
    "    * *temporal difference* equation.\n",
    "    * V(s) (expected return over a trajectory from a given state $s$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TBVILS(EndiControllerBase):\n",
    "    \"\"\" Implementation of Trajectory Based Value Iteration with LS minimization\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, system, horizon_length=10, **kwargs):\n",
    "        super(TBVILS, self).__init__(system, **kwargs)\n",
    "        self.ctrl_mode = 3\n",
    "        self.x_buffer = self.y_buffer # renaming y to x to stick to common notation\n",
    "        self.horizon_length = horizon_length\n",
    "        self.gamma = 0.95\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "        # control bounds tiled over horizon_length\n",
    "        self.u_min = np.tile(self.min_bounds, self.horizon_length)\n",
    "        self.u_max = np.tile(self.max_bounds, self.horizon_length)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\" initialize weights for parameter vector `W` \"\"\"\n",
    "\n",
    "        num_weights = int(self.dim_state + self.dim_input)\n",
    "\n",
    "        self.W = np.random.rand(num_weights)\n",
    "        self.w_min = np.zeros(num_weights)\n",
    "        self.w_max = 1e3 * np.ones(num_weights)\n",
    "    \n",
    "    def _update_buffers(self, u, x):\n",
    "        \"\"\" update x and u buffers on each call of compute_action\n",
    "\n",
    "        Args:\n",
    "\n",
    "            u : float vector\n",
    "                * control (action)\n",
    "\n",
    "            x : float vector\n",
    "                * state\n",
    "\n",
    "        \"\"\"\n",
    "        self.u_buffer = np.vstack([self.u_buffer, u])[-self.buffer_size:, :]\n",
    "        self.x_buffer = np.vstack([self.x_buffer, x])[-self.buffer_size:, :]\n",
    "        \n",
    "    def compute_action(self, t, x):\n",
    "        \"\"\" compute action for time step\n",
    "\n",
    "        Args:\n",
    "\n",
    "            t : float\n",
    "                * time step - current time step\n",
    "            \n",
    "            x : float vector\n",
    "                * state\n",
    "\n",
    "        Returns:\n",
    "            u : vector\n",
    "                * new action/control\n",
    "\n",
    "        \"\"\"\n",
    "        time_since_last_action = t - self.ctrl_clock\n",
    "\n",
    "        if time_since_last_action >= self.sample_time:\n",
    "            # Update controller's internal clock\n",
    "            self.ctrl_clock = t\n",
    "\n",
    "            ### YOUR SOLUTION BELOW\n",
    "            # Line 6: update buffers (call method above)\n",
    "                 \n",
    "            # Line 7: update Q-function (call weight_update method)\n",
    "\n",
    "            # Line 9: update policy function (call policy_method)\n",
    "\n",
    "            return self.u_curr\n",
    "            ### YOUR SOLUTION ABOVE\n",
    "\n",
    "        else:\n",
    "            return self.u_curr\n",
    "\n",
    "    def _policy(self, x, W):\n",
    "        \"\"\" calculate next action for state\n",
    "\n",
    "        Args:\n",
    "\n",
    "            x : float vector\n",
    "\n",
    "        Returns:\n",
    "        \n",
    "            u : vector\n",
    "                * new action/control\n",
    "\n",
    "        \"\"\"\n",
    "        # define solver options\n",
    "        options = {'maxiter': 200, 'disp': False, 'ftol': 1e-7}\n",
    "        bounds = sp.optimize.Bounds(self.u_min, self.u_max, keep_feasible=True)\n",
    "\n",
    "        # tile current action across buffer size to make trajectory\n",
    "        u_trajectory = np.tile(self.u_curr, self.horizon_length)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Minimize cost-over-trajectory with respect to actions. Returns a trajectory of actions.\n",
    "        You'll need to study how this works by reading the documentation. The variable `u_trajectory` \n",
    "        serves as an initial input of actions used by `minimize` to minimize \n",
    "        the cost by updating parameters.\n",
    "        \"\"\"\n",
    "        \n",
    "        U_new = minimize(lambda U: self._cost_over_traj(U, W, x), u_trajectory,\n",
    "                     method='SLSQP',\n",
    "                     tol=1e-7,\n",
    "                     bounds=bounds,\n",
    "                     options=options).x\n",
    "\n",
    "        # output first action of new action (trajectory)\n",
    "        first_u = U_new[:self.dim_input]\n",
    "\n",
    "        return first_u\n",
    "\n",
    "    def _cost_over_traj(self, u, w, x):\n",
    "        \"\"\" generate trajectory of steps and calculate cost over the trajectory\n",
    "\n",
    "        Description: given initial action (u) and observtion (x), create a trajectory of predicted steps/states, where the number of predictions == horizon_length. Then, calculate the discounted cost of this trajectory with the Q-function.\n",
    "\n",
    "        Args:\n",
    "\n",
    "            u : float vector\n",
    "                * control/action\n",
    "            \n",
    "            w : float vector\n",
    "                * Q-function parameters\n",
    "            \n",
    "            x : float vector\n",
    "                * state\n",
    "\n",
    "        Returns: \n",
    "\n",
    "            trajectory_q_cost : float\n",
    "\n",
    "        \"\"\"\n",
    "        u_trajectory = np.reshape(u, (self.horizon_length, self.dim_input))\n",
    "        x_trajectory = np.zeros([self.horizon_length, self.dim_output])\n",
    "        x_trajectory[0, :] = x\n",
    "\n",
    "        for k in range(1, self.horizon_length):\n",
    "            x = x + self.step_size * self.sys_dynamics(None, x, u_trajectory[k - 1, :], self.m, self.I, self.dim_state, self.is_disturb)\n",
    "\n",
    "            x_trajectory[k, :] = x\n",
    "\n",
    "        ### YOUR SOLUTION BELOW\n",
    "        # calculate cost of trajectory by calling the value function. `calculate_td` should be false.\n",
    "\n",
    "        ### YOUR SOLUTION ABOVE\n",
    "\n",
    "        return traj_q_cost\n",
    "\n",
    "    def _weight_update(self, Winit, u_buffer, x_buffer):\n",
    "        \"\"\" update weights for Q-function.\n",
    "\n",
    "        Args:\n",
    "\n",
    "            Winit : float vector\n",
    "                * initial weights for solver to start minimizing from\n",
    "\n",
    "            u_buffer : 2D float array\n",
    "                * controls buffer\n",
    "\n",
    "            x_buffer : 2D float array\n",
    "                * state buffer\n",
    "\n",
    "        Returns:\n",
    "\n",
    "            W_new - updated weights\n",
    "\n",
    "        \"\"\"\n",
    "        ### YOUR SOLUTION BELOW\n",
    "        # your solution should look very similar to the policy update.\n",
    "\n",
    "        ### YOUR SOLUTION ABOVE\n",
    "\n",
    "    def _value_function(self, W, u_container, x_container, length, calculate_td=False):\n",
    "        \"\"\" Q-function\n",
    "\n",
    "        Args:\n",
    "\n",
    "            W : float vector\n",
    "                * Q-function parameters\n",
    "\n",
    "            u_container : 2D float array\n",
    "                * buffer or trajectory of control inputs\n",
    "\n",
    "            x_container : 2D float array\n",
    "                * buffer or trajectory of observations\n",
    "            \n",
    "            calculate_td : boolean\n",
    "                * calculcate V(S) or temporal difference?\n",
    "\n",
    "        Returns:\n",
    "            J : float\n",
    "                * cost\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        ### YOUR SOLUTION BELOW\n",
    "\n",
    "        J = 0\n",
    "\n",
    "        for k in range(1, length):\n",
    "            x = x_container[k - 1, :]\n",
    "            u = u_container[k - 1, :]\n",
    "            x_next = x_container[k, :]\n",
    "            u_next = u_container[k, :]\n",
    "\n",
    "            if calculate_td:\n",
    "                # calculate temporal difference (don't forget to increment J)\n",
    "\n",
    "            else:\n",
    "                # calculate cost over trajectory (don't forget to increment J)\n",
    "\n",
    "        ### YOUR SOLUTION ABOVE\n",
    "        return J\n",
    "\n",
    "    def _phi(self, x, u=None):\n",
    "        \"\"\" Feature vector used in approximating Q\n",
    "\n",
    "        Args:\n",
    "\n",
    "            x : float vector\n",
    "                * state\n",
    "\n",
    "            u : float vector\n",
    "                * controls\n",
    "\n",
    "        returns:\n",
    "            chi : vector\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        ### YOUR SOLUTION BELOW\n",
    "        # if u is not None:\n",
    "            # chi = concatenate(x, u)\n",
    "        # else:\n",
    "            # chi = x\n",
    "        ### YOUR SOLUTION ABOVE\n",
    "\n",
    "        return chi**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run simulation\n",
    "\n",
    "Run the cell below to conduct training.\n",
    "* Note if the table rows overrun onto other rows, it's because of the width of jupyter notebook page. If you run this from terminal, the table appears correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create system\n",
    "sys = EndiSystem(initial_x=7, initial_y=7)\n",
    "\n",
    "# create agent\n",
    "agent = TBVILS(sys, sample_time=0.3, t1=17)\n",
    "\n",
    "# create sim\n",
    "sim = Simulation(sys, agent)\n",
    "\n",
    "sim.run_simulation(n_runs=2, \n",
    "                is_visualization=False, \n",
    "                close_plt_on_finish=False, \n",
    "                show_annotations=True, \n",
    "                print_summary_stats=False, \n",
    "                print_statistics_at_step=True,\n",
    "                print_inline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you choose to run the visual training option below, you can print summary statistics with `sim.print_sim_summary_stats()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment to run visual training option\n",
    "# %matplotlib notebook\n",
    "# HTML(sim.run_simulation(n_runs=2, fig_width=7, fig_height=7, show_annotations=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading</font>\n",
    "Run this cell to track your answers and to save your answer for problem 1.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADING DO NOT MODIFY\n",
    "statistics, = sim.final_statistics\n",
    "\n",
    "hw2_answers.record('problem_1-2', {'mean_rc': statistics[0],\n",
    "    'mean_velocity': statistics[1], \n",
    "    'sd_rc': statistics[2],\n",
    "    'sd_velocity': statistics[3],\n",
    "    'sd_alpha': statistics[4],\n",
    "    'l2_norm': statistics[5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerations\n",
    "\n",
    "Linear approximation is a fundamental method by which real-life RL problems are solved. This assignment showed you the basis of how they work and what their mathematical forms are. Please get comfortable with the basics of linear algebra, matrix calculus, and Python as we proceed to more diverse challenges in the coming assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading: Submit your answers</font>\n",
    "Enter your first and last name in the cell below and then run it to save your answers for this lab to a JSON file. The file is saved to the same directory as this notebook. After the file is created, upload the JSON file to the assignment page on Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw2_answers.print_answers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignment_name = \"hw_2\"\n",
    "first_name = \"YOUR_FIRST_NAME\" # Use proper capitalization\n",
    "last_name = \"YOUR_LAST_NAME\" # Use proper capitalization\n",
    "\n",
    "hw2_answers.save_to_json(assignment_name, first_name, last_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions?\n",
    "\n",
    "Reach out to your instructors on Piazza.\n",
    "\n",
    "### Sources\n",
    "\n",
    "***\n",
    "\n",
    "<sup>[1]</sup> Geramifard, A., Walsh, T., Tellex, S., Chowdhary, G., Roy, N., & How, J.P. (2013). A Tutorial on Linear Function Approximators for Dynamic Programming and Reinforcement Learning. Found. Trends Mach. Learn., 6, 375-451.\n",
    "\n",
    "<sup>[2]</sup> Busoniu, L., Babu≈°ka, R., Schutter, B.D., & Ernst, D. (2010). Reinforcement Learning and Dynamic Programming Using Function Approximators.\n",
    "\n",
    "<sup>[3]</sup> Lewis, F., & Vrabie, D. (2009). Reinforcement learning and adaptive dynamic programming for feedback control. IEEE Circuits and Systems Magazine, 9, 32-50."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
