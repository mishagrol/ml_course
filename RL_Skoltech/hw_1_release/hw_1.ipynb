{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"skoltech_logo.png\" alt=\"Skoltech\" width=60% height=60% />\n",
    "<h1 style=\"color:#333333; text-align:center; line-height: 0;\">Reinforcement Learning</h1>\n",
    "<h5 style=\"color:#333333; text-align:center;\">Course MA030422</h5>\n",
    "\n",
    "<h2 style=\"color:#A7BD3F;\">Homework 1</h2>\n",
    "\n",
    "***\n",
    "\n",
    "Welcome to homework 1 of the *Reinforcement Learning* course at Skoltech CDISE! In this homework we will be covering two types of classical approaches to RL problems: **value iteration** and **policy iteration**.\n",
    "\n",
    "### Components\n",
    "\n",
    "* **Section 1**: Review of relevant concepts\n",
    "* **Section 2**: OpenAI FrozenLake environment\n",
    "* **Section 3**: Introduction to algorithms in DP for finite MDPs\n",
    "    * Exercise 1\n",
    "        * Problem 1.1 - Value Iteration (15 points)\n",
    "        * Problem 1.2 - Policy Iteration (15 points)\n",
    "        * Problem 1.3 - FrozenLake8x8 (2 points)\n",
    "        \n",
    "Total points: 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\">Section 1</h2>\n",
    "\n",
    "***\n",
    "\n",
    "### Background material \n",
    "\n",
    "Before we begin, let's refresh our memory on this question: what is the core problem that is being solved for in reinforcement learning? \n",
    "\n",
    "At the simplest level, the problem we are solving for is to teach the agent to behave *optimally* in a specific environment. An example might be to teach a robot to bounce a ball for some period of time; or program a helicopter to keep the same altitude in unpredictable windy conditions.\n",
    "\n",
    "The numerical definition of what is *optimal* is defined by the **objective function**, which is typically maximized in the context of MDPs (Markov Decision Processes). In the context of MDPs , this objective function is known as the **value function**, aka the *total expected reward*, which is received over sequential state transitions.\n",
    "\n",
    "The goal then, is to teach the agent to maximize the *total expected reward* it earns over some time horizon (theoretically, it is an infinite time horizon) by selecting the best action as dictated by the value function. The agent learns to maximize rewards as it transitions from state to state, taking actions in each state. In a deterministic case, the transitioning process is diagrammed as follows:\n",
    "\n",
    "$$\\text{State 1}\\xrightarrow[Action]{}\\text{State 2} \\xrightarrow[Action]{}\\text{State 3}\\xrightarrow[Action]{}\\dots$$\n",
    "\n",
    "### Markov Decision Processes\n",
    "\n",
    "Let's formalize the key components of the RL problem in the context of MDPs:\n",
    "\n",
    "An MDP is defined by: $(S, A, P, R, S_0, \\gamma)$\n",
    "* $S$ = set of states (state-space)\n",
    "* $A$ = set of actions (action-space)\n",
    "* $P$ = state transition probabilities\n",
    "* $R$ = reward for taking an action $a\\in\\text{A}$ in state $s\\in\\text{S}$\n",
    "* $S_0$ = starting state\n",
    "* $\\gamma$ = discount rate\n",
    "    \n",
    "In more detail:\n",
    "* **States** - states can be discrete/finite (imagine cells in a grid world) or continuous/infinite (position on a road).\n",
    "    * Referred to as the *state space* (i.e. discrete state space or continuous state space)\n",
    "* **Actions** - actions can also be discrete (moving up/down/left/right in a grid world cell) or continuous (how many degrees to turn a steering wheel when driving a car).\n",
    "    * Referred to as the *action space* (i.e. discrete action space or continuous action space)\n",
    "* **Rewards** - rewards are issued by a reward function $\\rho : S_t \\times A_t \\rightarrow R$. The reward function is a property of the environment.\n",
    "* **Transition probabilities**. In MDPs, this is denoted by $P_{s,a}$. The transition probability is the probability that, for example, some action $A$ in state $S$ leads to state $S^\\prime$ (prime denotes the next time step) - represented notationally as $p(s^\\prime|s,a)$.\n",
    "* **Discount factor** - the discount factor is a number greater than 0 and less than 1 that is used to discount rewards received over sequential time-steps. It is denoted as $\\gamma \\in [0, 1)$\n",
    "* **Value function** - one of the primary functions learned by the agent: the value function dictates either the value of a state or the value of action. More on this below.\n",
    "* **Policy function** - one of the primary functions learned by the agent: the policy maps states to actions. More below.\n",
    "\n",
    "#### Other useful definitions\n",
    "* **Experience** - $\\big(\\text{State}_{t}$, $\\text{Action}_{t}$, $\\text{Reward}_{t}\\big)$ tuple\n",
    "* **Trajectory** - A sequence of *experiences* through time, represented as: $\\tau$ (tau)\n",
    "* **Episode** - A trajectory that ends in a terminal state\n",
    "\n",
    "\n",
    "### Policy\n",
    "\n",
    "The process of learning for the agent can be thought of as a sequence of mapping states to actions $a = \\pi(s)$ to maximize expected reward over an episode. This is known as the **policy:** $\\pi: S \\rightarrow A$. \n",
    "\n",
    "Note:\n",
    "* The agent needs to explore and interact with its environment to learn where actions earn the maximum rewards.\n",
    "* Actions in the current time step effect rewards in future time steps\n",
    "* There is a trade off between the frequency of sampling the environment and frequency of taking actions\n",
    "* It can be based on discrete state-spaces or continuous state-spaces (and same for action-spaces)\n",
    "\n",
    "### Value functions\n",
    "\n",
    "The **worthiness** of a policy is calculated by the aforementioned *value function*. There are various forms of value functions. First, the value of a state <sup>[3]</sup>:\n",
    "\n",
    "<img src=\"state_to_action.png\" width=\"65%\" height=\"65%\" />\n",
    "\n",
    "Second, the value of action <sup>[3]</sup>:\n",
    "\n",
    "<img src=\"action_to_state.png\" width=\"55%\" height=\"55%\" />\n",
    "\n",
    "Let's review the notation of these value functions in terms of *expected value/reward* (aka expectation notation) <sup>[3][4]</sup>. Note:\n",
    "* $\\tau$ = trajectory\n",
    "\n",
    "#### <font color=\"#1DAE00\">On-policy ($\\pi$) state-value function</font> :\n",
    "\n",
    "$$V^\\pi{(s)} = \\displaystyle \\mathop{\\mathbb{E}}_{\\tau\\sim \\pi}\\big[R(\\tau)\\thinspace|\\thinspace s_0 = s\\big]$$\n",
    "\n",
    "* **Definition**: the expected reward of trajectory $\\tau$ (sampled from policy $\\pi$) starting from state $s$ *over an infinite time horizon*.\n",
    "\n",
    "#### <font color=\"#1DAE00\">Optimal ($*$) state-value function</font> :\n",
    "\n",
    "$$V^*{(s)} = \\max_{\\pi}{\\displaystyle \\mathop{\\mathbb{E}}_{\\tau\\sim \\pi}\\big[R(\\tau)\\thinspace|\\thinspace s_0 = s\\big]}$$\n",
    "\n",
    "* **Definition**: the <font color=\"red\">maximum (of all policies)</font> expected reward of trajectory $\\tau$ (sampled from policy $\\pi$) starting from state $s$ *over an infinite time horizon*.\n",
    "\n",
    "#### <font color=\"#5A00AE\">On-policy ($\\pi$) action-value function</font> :\n",
    "\n",
    "$$Q^\\pi{(s, a)} = \\displaystyle \\mathop{\\mathbb{E}}_{\\tau\\sim \\pi}\\big[R(\\tau)\\thinspace|\\thinspace s_0 = s, a_0 = a\\big]$$\n",
    "\n",
    "* **Definition**: the expected reward of trajectory $\\tau$ (sampled from policy $\\pi$) starting from state $s$ and taking action $a$ *over an infinite time horizon*.\n",
    "\n",
    "#### <font color=\"#5A00AE\">Optimal ($*$) action-value function</font> :\n",
    "\n",
    "$$Q^*{(s,a)} = \\max_{\\pi}{\\displaystyle \\mathop{\\mathbb{E}}_{\\tau\\sim \\pi}\\big[R(\\tau)\\thinspace|\\thinspace s_0 = s, a_0 = a\\big]}$$\n",
    "\n",
    "* **Definition**: the <font color=\"red\">maximum (of all policies)</font> expected reward of trajectory $\\tau$ (sampled from policy $\\pi$) starting from state $s$  and taking action $a$ *over an infinite time horizon*.\n",
    "\n",
    "\n",
    "#### Bellman equations:\n",
    "\n",
    "Each of the value functions above has a slightly more explicit form that defines how value is calculated recursively. These are known as **Bellman's equations** <sup>[3][4]</sup>: \n",
    "\n",
    "#### <font color=\"#DE001B\">Bellman's on-policy ($\\pi$) state-value function</font> :\n",
    "\n",
    "$$V^\\pi{(s)} = \\displaystyle \\mathop{\\mathbb{E}}_{\\substack{a\\sim\\pi\\\\s^\\prime\\sim P}}\\big[R(s,a) + \\gamma V^\\pi{(s^\\prime)}\\big]$$\n",
    "\n",
    "#### <font color=\"#DE001B\">Bellman's optimal ($*$) state-value function</font> :\n",
    "\n",
    "$$V^*{(s)} = \\displaystyle \\max_{a}{\\mathop{\\mathbb{E}}_{s^\\prime\\sim{P}}\\big[R(s,a) + \\gamma V^*{(s^\\prime)}\\big]}$$\n",
    "\n",
    "#### <font color=\"#A1AE00\">Bellman's on-policy ($\\pi$) action-value function</font> :\n",
    "\n",
    "$$Q^\\pi{(s,a)} = \\displaystyle \\mathop{\\mathbb{E}}_{s^\\prime\\sim P}\\big[R(s,a) + \\gamma \\mathop{\\mathbb{E}}_{a^\\prime\\sim{\\pi}}[Q^\\pi{(s^\\prime, a^\\prime)}]\\big]$$\n",
    "\n",
    "#### <font color=\"#A1AE00\">Bellman's optimal ($*$) action-value function</font> :\n",
    "\n",
    "$$Q^*{(s,a)} = \\displaystyle \\mathop{\\mathbb{E}}_{s^\\prime\\sim{P}}\\big[R(s,a) + \\gamma \\max_{a^\\prime}Q^*{(s^\\prime, a^\\prime)}\\big]$$\n",
    "\n",
    "\n",
    "### Expanded notations of Bellman's functions\n",
    "\n",
    "Going further, all of the value functions above can be expressed more explicitly. These are the equations that you need to know for programming purposes <sup>[3][2]</sup>:\n",
    "\n",
    "#### <font color=\"#00DED1\">Bellman's on-policy ($\\pi$) state-value function</font> :\n",
    "\n",
    "$$V^\\pi{(s)} = \\sum_{a^\\prime\\in\\text{A}} \\pi(a|s) \\sum_{s^\\prime\\in\\text{S}}P{(s^\\prime|s,a)} \\big[R{(s,a,s^\\prime)} + \\gamma V^\\pi{(s^\\prime)}\\big]$$\n",
    "\n",
    "#### <font color=\"#00DED1\">Bellman's optimal ($*$) state-value function</font> :\n",
    "\n",
    "$$V^*{(s)} = \\max_{a^\\prime\\in\\text{A}} \\sum_{s^\\prime\\in\\text{S}}P{(s^\\prime|s,a)} \\big[R{(s,a,s^\\prime)} + \\gamma V^*{(s^\\prime)}\\big]$$\n",
    "\n",
    "#### <font color=\"#DE008A\">Bellman's on-policy ($\\pi$) action-value function</font> :\n",
    "\n",
    "$$Q^\\pi{(s,a)} = \\sum_{s^\\prime\\in\\text{S}} \\big[R{(s,a,s^\\prime)} + \\gamma \\sum_{a^\\prime\\in\\text{A}} \\pi{(a^\\prime|s^\\prime)} Q^\\pi{(s^\\prime,a^\\prime)}\\big]$$\n",
    "\n",
    "#### <font color=\"#DE008A\">Bellman's optimal ($*$) action-value function</font> :\n",
    "\n",
    "$$Q^*{(s,a)} = \\sum_{s^\\prime\\in\\text{S}} P{(s^\\prime|s,a)}  \\big[R{(s,a,s^\\prime)} + \\gamma \\max_{a^\\prime} Q^*{(s^\\prime,a^\\prime)}\\big]$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Goal of this homework\n",
    "\n",
    "In this homework we will practice understanding these value functions as they are applied to classical iteration algorithms in RL (specifically finite MDPs). Be sure that you installed OpenAI gym for Python [here](https://gym.openai.com/docs/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\">Section 2 - Environment</h2>\n",
    "\n",
    "***\n",
    "\n",
    "### Intro to OpenAI <i style=\"color:blue;\">FrozenLake</i> environment\n",
    "\n",
    "For this homework we will be exploring agent training in grid world environment called *FrozenLake*. Read more about it [here](https://gym.openai.com/envs/FrozenLake-v0/). To summarize:\n",
    "* FrozenLake is a 2D grid world\n",
    "* There are 2 variants of the environment: 4x4 and 8x8. We will try both.\n",
    "* There are 4 types of grid cells (S, F, H, G). $S$ is the starting point, $G$ is the goal, $F$ is a frozen surface and $H$ is a hole.\n",
    "* If an agent steps onto a slippery surface, he may slip and not end up in the next desired state for which he took an action (think, transition probabilities!).\n",
    "* The rewards are sparse: the agent receives a reward of 1 when reaching the goal and 0 otherwise. If the agent falls into a hole, the episode is over.\n",
    "* <font color=\"red\">Note: we will not be using the slippery version of the environment</font> for the purposes of simpler introduction to core concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\">Section 3 - Dynamic Programming algorithms for MDPs</h2>\n",
    "\n",
    "***\n",
    "\n",
    "In environments that have ‚ùó<font color=\"red\">discrete</font> state **and** action spaces, such as FrozenLake or Gym-Minigrid (which we will see in lab 2), two classical algorithms can be used to train the RL agent to solve the environment's goal.\n",
    "\n",
    "The first is **value iteration** <sup>[3]</sup>:\n",
    "\n",
    "<img src=\"value_iteration.png\" alt=\"Value Iteration\" width=65% height=65% />\n",
    "\n",
    "The second is **policy iteration** <sup>[3]</sup>:\n",
    "\n",
    "<img src=\"policy_iteration.png\" alt=\"Policy Iteration\" width=65% height=65% />\n",
    "\n",
    "Below, we have implemented a class called `MDP` that combines operations common to **both** of these algorithms. \n",
    "\n",
    "#### <font color=\"red\">First</font>, read pages pg. 74 through 83 in Chapter 4 of your class text <sup>[3]</sup>.\n",
    "\n",
    "#### Next, examine the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import collections\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "\"\"\"\n",
    "DO NOT MODIFY\n",
    "\"\"\"\n",
    "class MDP:\n",
    "    def __init__(self, env_name, is_slippery=False):\n",
    "        self.env = gym.make(env_name, is_slippery=is_slippery)\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        self.transits = collections.defaultdict(collections.Counter)\n",
    "        self.values = collections.defaultdict(float)\n",
    "        self.gamma = 0.95\n",
    "        self.theta = 0.0005\n",
    "\n",
    "    def return_rewards(self):\n",
    "        return 1.0 in self.rewards.values()\n",
    "        \n",
    "    def return_state_values(self):\n",
    "        return tuple(self.values.items())\n",
    "\n",
    "    def _model_transits_rewards(self, num_steps):\n",
    "        \"\"\"\n",
    "\n",
    "        Description: step through the environment to model rewards and transits for all states. Also called \"filling a buffer\".\n",
    "\n",
    "        Args:\n",
    "            * num_steps - num steps to take through env. Should be sufficient to stochastically achieve goal of 1.\n",
    "        \"\"\"\n",
    "        current_state = self.env.reset()\n",
    "\n",
    "        print(\"Modeling rewards and transition probabilities ...\")\n",
    "    \n",
    "        for i in tqdm(range(num_steps)):\n",
    "            # sample random action\n",
    "            action = self.env.action_space.sample()\n",
    "\n",
    "            # take step\n",
    "            new_state, reward, is_done, _ = self.env.step(action)\n",
    "            \n",
    "            # assign rewards for new state\n",
    "            self.rewards[(current_state, action, new_state)] = reward\n",
    "\n",
    "            # log transit from state to new state\n",
    "            self.transits[(current_state, action)][new_state] += 1\n",
    "            \n",
    "            if is_done:\n",
    "                current_state = self.env.reset()\n",
    "            else:\n",
    "                current_state = new_state\n",
    "\n",
    "    def _get_state_value(self, current_state, action):\n",
    "        \"\"\" \n",
    "\n",
    "        Description: Get the value of state. see Bellman equation for state-value in assignment. This is the sum of the discounted, expected reward at state-prime, for all states-prime from the current state.\n",
    "\n",
    "        Args:\n",
    "            * Current state\n",
    "            * Current action (policy)\n",
    "\n",
    "        Returns:\n",
    "            * Value of current state\n",
    "\n",
    "        \"\"\"\n",
    "        next_state_counts = self.transits[(current_state, action)]\n",
    "        \n",
    "        total_transits = sum(next_state_counts.values())\n",
    "        current_state_value = 0.0\n",
    "        \n",
    "        print('next_State_counts:',next_state_counts)\n",
    "        for next_state, n_transits in next_state_counts.items():\n",
    "            reward = self.rewards[(current_state, action, next_state)]\n",
    "            transit_prob = (n_transits / total_transits)\n",
    "            current_state_value += transit_prob * (reward + self.gamma * self.values[next_state])\n",
    "        \n",
    "        return current_state_value\n",
    "\n",
    "    def _get_best_action(self, state):\n",
    "        \"\"\" \n",
    "        \n",
    "        Description: get best action for current state\n",
    "\n",
    "        Args:\n",
    "            * state\n",
    "\n",
    "        Returns:\n",
    "            * best action for current state\n",
    "\n",
    "        \"\"\"\n",
    "        action_values = {}\n",
    "\n",
    "        for action in range(self.env.action_space.n):\n",
    "            state_value = self._get_state_value(state, action)\n",
    "            \n",
    "            action_values[action] = state_value\n",
    "\n",
    "        best_action_value = max(action_values.values())\n",
    "        best_action = max(action_values, key=action_values.get)\n",
    "        \n",
    "        return best_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Exercise 1: Classical MDP algorithms</font>\n",
    "\n",
    "### <font color=\"blue\">Problem 1.1 - Value Iteration</font>\n",
    "\n",
    "#### üéØ Task:  Implement Value Iteration on the FrozenLake environment.\n",
    "\n",
    "Guidance/hints:\n",
    "* Read Chapter 4 from the class text.\n",
    "* Be sure you understand the algorithm (value iteration in this case).\n",
    "* Test taking steps through the FrozenLake environment by making your own script and executing it in new code cells\n",
    "* Explore the MDP class above\n",
    "* To get it work, you'll need to make sure your algorithm is properly calculating the value of states. Call the `return_state_values` method to check the value of states. The value of the final state (aka the goal state) is always 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first is **value iteration** <sup>[3]</sup>:\n",
    "\n",
    "<img src=\"value_iteration.png\" alt=\"Value Iteration\" width=65% height=65% />\n",
    "\n",
    "The second is **policy iteration** <sup>[3]</sup>:\n",
    "\n",
    "<img src=\"policy_iteration.png\" alt=\"Policy Iteration\" width=65% height=65% />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 0.0),)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 1797/100000 [00:00<00:05, 17968.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling rewards and transition probabilities ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100000/100000 [00:03<00:00, 29019.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {(0, 0): Counter({0: 10764}),\n",
       "             (0, 2): Counter({1: 10760}),\n",
       "             (1, 0): Counter({0: 4359}),\n",
       "             (0, 3): Counter({0: 10556}),\n",
       "             (0, 1): Counter({4: 10599}),\n",
       "             (4, 1): Counter({8: 4027}),\n",
       "             (8, 3): Counter({4: 1569}),\n",
       "             (8, 0): Counter({8: 1519}),\n",
       "             (8, 1): Counter({12: 1517}),\n",
       "             (1, 3): Counter({1: 4185}),\n",
       "             (4, 0): Counter({4: 4136}),\n",
       "             (4, 3): Counter({0: 4065}),\n",
       "             (1, 2): Counter({2: 4122}),\n",
       "             (2, 1): Counter({6: 1861}),\n",
       "             (6, 3): Counter({2: 559}),\n",
       "             (2, 3): Counter({2: 1862}),\n",
       "             (6, 1): Counter({10: 522}),\n",
       "             (10, 1): Counter({14: 302}),\n",
       "             (14, 0): Counter({13: 159}),\n",
       "             (13, 2): Counter({14: 211}),\n",
       "             (14, 3): Counter({10: 176}),\n",
       "             (10, 2): Counter({11: 308}),\n",
       "             (2, 0): Counter({1: 1885}),\n",
       "             (1, 1): Counter({5: 4164}),\n",
       "             (4, 2): Counter({5: 4076}),\n",
       "             (8, 2): Counter({9: 1465}),\n",
       "             (9, 3): Counter({5: 468}),\n",
       "             (6, 0): Counter({5: 543}),\n",
       "             (6, 2): Counter({7: 525}),\n",
       "             (10, 3): Counter({6: 288}),\n",
       "             (14, 1): Counter({14: 154}),\n",
       "             (14, 2): Counter({15: 178}),\n",
       "             (2, 2): Counter({3: 1874}),\n",
       "             (3, 2): Counter({3: 908}),\n",
       "             (3, 1): Counter({7: 935}),\n",
       "             (9, 1): Counter({13: 502}),\n",
       "             (13, 0): Counter({12: 220}),\n",
       "             (9, 2): Counter({10: 500}),\n",
       "             (9, 0): Counter({8: 525}),\n",
       "             (13, 1): Counter({13: 236}),\n",
       "             (13, 3): Counter({9: 230}),\n",
       "             (10, 0): Counter({9: 300}),\n",
       "             (3, 3): Counter({3: 947}),\n",
       "             (3, 0): Counter({2: 939})})"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent1 = ValueIteration(\"FrozenLake-v0\")\n",
    "agent1._model_transits_rewards(100000)\n",
    "agent1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-123-f7819b9166de>, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-123-f7819b9166de>\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    v=self._get_state_value(state, self._get_best_action(state))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ADD YOUR CODE BETWEEN THE COMMENTS BELOW\n",
    "\"\"\"\n",
    "class ValueIteration(MDP):\n",
    "    def __init__(self, env_name, is_slippery=False):\n",
    "        super().__init__(env_name, is_slippery=is_slippery)\n",
    "        \n",
    "    def _value_iteration(self):\n",
    "        \"\"\" \n",
    "\n",
    "        Description: Perform Value Iteration\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"Performing value iteration ...\")\n",
    "\n",
    "        delta = 0\n",
    "        iter_count = 1\n",
    "\n",
    "        while True:\n",
    "            ### YOUR SOLUTION BELOW\n",
    "\n",
    "\n",
    "            final_delta = delta / (iter_count * self.env.observation_space.n)\n",
    "            \n",
    "            iter_count += 1\n",
    "\n",
    "            if final_delta < self.theta:\n",
    "                break\n",
    "                \n",
    "    def _run_episode(self, render=True):\n",
    "        \"\"\"\n",
    "\n",
    "        Description: perform an episode on the environment\n",
    "\n",
    "        Args\n",
    "            * Render - render env to screen?\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        clear_output()\n",
    "        episode_reward = 0.0\n",
    "        current_state = self.env.reset()\n",
    "\n",
    "        if render:\n",
    "            self.env.render()\n",
    "        \n",
    "        while True:\n",
    "            action = self._get_best_action(current_state)\n",
    "            new_state, step_reward, is_done, _ = self.env.step(action)\n",
    "            \n",
    "            self.rewards[(current_state, action, new_state)] = step_reward\n",
    "            self.transits[(current_state, action)][new_state] += 1\n",
    "            \n",
    "            episode_reward += step_reward\n",
    "            \n",
    "            if render:\n",
    "                self.env.render()\n",
    "\n",
    "            if is_done:\n",
    "                self.env.reset()\n",
    "                break\n",
    "            \n",
    "            current_state = new_state\n",
    "\n",
    "        print(f\"...Episode completed.\")\n",
    "\n",
    "        return episode_reward\n",
    "        \n",
    "    def run_simulation(self, num_steps = 1000, render=True):\n",
    "        \"\"\" Run training simulation \"\"\"\n",
    "        try:\n",
    "            ### YOUR SOLUTION BELOW\n",
    "            #raise NotImplementedError # replace this with your solution\n",
    "            \n",
    "            for i in range(num_steps):\n",
    "                self._value_iteration()\n",
    "            \n",
    "            ### YOUR SOLUTION ABOVE\n",
    "            episode_reward = self._run_episode(render=render)\n",
    "\n",
    "            if episode_reward > 0.85:\n",
    "                print(f\"Environment solved.\")\n",
    "            else:\n",
    "                clear_output()\n",
    "                print(f\"Failed to solve environment.\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"...Cancelling...\")\n",
    "            \n",
    "        except NotImplementedError:\n",
    "            print(f\"Your solution is incomplete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #def _value_iteration(self):\n",
    "# \"\"\" \n",
    "\n",
    "# Description: Perform Value Iteration\n",
    "\n",
    "# \"\"\"\n",
    "# print(\"Performing value iteration ...\")\n",
    "\n",
    "# delta = 0\n",
    "# iter_count = 1\n",
    "# current_state = self.env.reset()\n",
    "# self._get_state_value(current_state)\n",
    "# # while True:\n",
    "# #     ### YOUR SOLUTION BELOW\n",
    "# #     #raise NotImplementedError # replace this with your solution\n",
    "# #     for state in States:\n",
    "# #         print(state)\n",
    "# #         v=self._get_state_value(current_state)\n",
    "# #         print(v)\n",
    "# #         V_s = self._get_state_value(current_state, self._get_best_action(current_state))\n",
    "# #         print(V_s)\n",
    "# #         delta = max(delta, v-V_s)\n",
    "\n",
    "# #     ### YOUR SOLUTION ABOVE\n",
    "\n",
    "# #     final_delta = delta / (iter_count * self.env.observation_space.n)\n",
    "\n",
    "# #     iter_count += 1\n",
    "\n",
    "# #     if final_delta < self.theta:\n",
    "# #         break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run simulation\n",
    "\n",
    "Execute the cell below. If your code is correct, you will see the environment render the agent taking steps. The final cell will be 'G'. \n",
    "\n",
    "* **Note**: Running the simulation below with a large enough `num_steps` hyperparameter is vital to the value iteration algorithm working successfully. This is because the modeling process, which is stochastic (via `_model_transits_rewards`), requires sufficient iterations to reach the goal-state and acquire the reward in the environment, as well as to make enough transits to all possible states to make value iteration numerically stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"FrozenLake-v0\"\n",
    "env = gym.make(env_name, is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to solve environment.\n",
      "Duration of execution: 21.51717 seconds\n"
     ]
    }
   ],
   "source": [
    "# DO NOT MODIFY\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "agent1 = ValueIteration(\"FrozenLake-v0\")\n",
    "agent1.run_simulation(num_steps = 100)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Duration of execution: {end_time-start_time:.5f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 0.0, False, {'prob': 1.0})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_state = env.reset()\n",
    "action=13\n",
    "ll = agent1._get_state_value(current_state, action=action)\n",
    "ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State values?\n",
    "\n",
    "In a 4x4 grid of FrozenLake, there are 16 states. Now let's look at their values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 0.0),)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_values = agent1.return_state_values()\n",
    "state_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading</font>\n",
    "Run this cell to track your answers and to save your answer for problem 1.1. Make sure you defined the necessary variable above to avoid a `NameError` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADING DO NOT MODIFY\n",
    "from grading_utilities import AnswerTracker\n",
    "hw1_answers = AnswerTracker()\n",
    "rewards_values = agent1.return_rewards()\n",
    "hw1_answers.record('problem_1-1', {'state_values': state_values, 'rewards': rewards_values})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Problem 1.2 - Policy Iteration</font>\n",
    "\n",
    "Next, we'll take on policy iteration.\n",
    "\n",
    "Look at the policy iteration algorithm at the start of section 3. How is it different from value iteration? Let's see:\n",
    "\n",
    "<img src=\"vipi_comparison.png\" width=\"90%\" height=\"90%\" />\n",
    "\n",
    "Indeed, these algorithms have a very similar value-of-state estimation cycle. The difference being: the *value iteration* algorithm iterates over **every** action, calculates expected reward of $s^\\prime$, and selects the maximum value action; while policy iteration calculates expected reward of $s^\\prime$ for the **single** action **specified by the policy**.\n",
    "\n",
    "#### üéØ Task:  Implement Policy Iteration on the FrozenLake environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration(MDP):\n",
    "    def __init__(self, env_name, is_slippery=False):\n",
    "        super().__init__(env_name, is_slippery=is_slippery)\n",
    "        self.policy = collections.defaultdict(int)\n",
    "        \n",
    "    def return_policy(self):\n",
    "        return tuple(self.policy.items())\n",
    "        \n",
    "    def _policy_iteration(self):\n",
    "        \"\"\" \n",
    "        \n",
    "        Description: Perform policy iteration. Consists of 2 parts: policy evaluation and policy improvement. See Sutton & Barto, RL: An Introduction, page 80.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        #-------------------#\n",
    "        # policy evaluation #\n",
    "        #-------------------#\n",
    "\n",
    "        delta = 0\n",
    "        iter_count = 1\n",
    "\n",
    "        while True:\n",
    "            ### YOUR SOLUTION BELOW\n",
    "            raise NotImplementedError # replace this with your solution\n",
    "            ### YOUR SOLUTION ABOVE\n",
    "\n",
    "            final_delta = delta / (iter_count * self.env.observation_space.n)\n",
    "            \n",
    "            iter_count += 1\n",
    "\n",
    "            if final_delta < self.theta:\n",
    "                break\n",
    "\n",
    "        #--------------------#\n",
    "        # policy improvement #\n",
    "        #--------------------#\n",
    "\n",
    "        policy_stable = np.zeros((self.env.observation_space.n), dtype=bool)\n",
    "\n",
    "        ### YOUR SOLUTION BELOW\n",
    "        raise NotImplementedError # replace this with your solution\n",
    "        ### YOUR SOLUTION ABOVE\n",
    "\n",
    "        return policy_stable\n",
    "    \n",
    "    def _run_episode(self, render=True):\n",
    "        \"\"\"\n",
    "\n",
    "        Description: runs an episode on the environment after policy iteration.\n",
    "\n",
    "        Args:\n",
    "            * Render - render env to screen?\n",
    "\n",
    "        \"\"\"\n",
    "        clear_output()\n",
    "        episode_reward = 0.0\n",
    "        current_state = self.env.reset()\n",
    "\n",
    "        if render:\n",
    "            self.env.render()\n",
    "        \n",
    "        while True:\n",
    "            action = self.policy[current_state]\n",
    "            new_state, step_reward, is_done, _ = self.env.step(action)\n",
    "            \n",
    "            self.rewards[(current_state, action, new_state)] = step_reward\n",
    "            self.transits[(current_state, action)][new_state] += 1\n",
    "            \n",
    "            episode_reward += step_reward\n",
    "            \n",
    "            if render:\n",
    "                self.env.render()\n",
    "\n",
    "            if is_done:\n",
    "                self.env.reset()\n",
    "                break\n",
    "            \n",
    "            current_state = new_state\n",
    "            \n",
    "        print(f\"...Episode completed.\")\n",
    "\n",
    "        return episode_reward\n",
    "        \n",
    "    def run_simulation(self, num_steps = 2000, render=True):\n",
    "        \"\"\" Run training simulation \"\"\"\n",
    "        try:\n",
    "            self._model_transits_rewards(num_steps)\n",
    "\n",
    "            while True:\n",
    "                policy_stable = self._policy_iteration()\n",
    "\n",
    "                if policy_stable.all() == True:\n",
    "                    break\n",
    "\n",
    "            episode_reward = self._run_episode(render=render)\n",
    "            \n",
    "            if episode_reward > 0.85:\n",
    "                print(f\"Environment solved.\")\n",
    "            else:\n",
    "                clear_output()\n",
    "                print(f\"Failed to solve environment.\")\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"...Cancelling...\")\n",
    "            \n",
    "        except NotImplementedError:\n",
    "            print(f\"Your solution is incomplete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run simulation\n",
    "\n",
    "As discussed above, beware of the num_steps hyperparam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "agent2 = PolicyIteration(\"FrozenLake-v0\")\n",
    "agent2.run_simulation(num_steps = 3000)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Duration of execution: {end_time-start_time:.5f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading</font>\n",
    "Run this cell to track your answers and to save your answer for problem 1.2. Make sure you defined the necessary variable above to avoid a `NameError` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_values = agent2.return_state_values()\n",
    "policy_values = agent2.return_policy()\n",
    "\n",
    "### GRADING DO NOT MODIFY\n",
    "hw1_answers.record('problem_1-2', {'state_values': state_values, 'policy_values': policy_values})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Problem 1.3 - FrozenLake8x8</font>\n",
    "\n",
    "Let's try the 8x8 version of FrozenLake and compare which algorithm is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# value iteration\n",
    "agent3 = ValueIteration(\"FrozenLake8x8-v0\")\n",
    "agent3.run_simulation(render = False, num_steps = 50000)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"VI: Duration of execution: {end_time-start_time:.5f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# policy iteration\n",
    "agent4 = PolicyIteration(\"FrozenLake8x8-v0\")\n",
    "agent4.run_simulation(render = False, num_steps = 50000)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"PI: Duration of execution: {end_time-start_time:.5f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which algorithm is faster?\n",
    "\n",
    "Which algorithm do you think is faster, on average, policy iteration (PI) or value iteration (VI)? The answer should be clear (if you implemented the algorithms correctly), but I encourage you to think about *why* (there are 2 reasons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Change None to 'PI' or 'VI' below\n",
    "\"\"\"\n",
    "### YOUR ANSWER BELOW\n",
    "problem_13_answer = None\n",
    "### YOUR ANSWER ABOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading</font>\n",
    "Run this cell to track your answers and to save your answer for problem 1.3. Make sure you defined the necessary variable above to avoid a `NameError` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADING DO NOT MODIFY\n",
    "hw1_answers.record('problem_1-3', problem_13_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading: Submit your answers</font>\n",
    "Enter your first and last name in the cell below and then run it to save your answers for this lab to a JSON file. The file is saved to the same directory as this notebook. After the file is created, upload the JSON file to the assignment page on Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hw1_answers.print_answers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "assignment_name = \"hw_1\"\n",
    "first_name = \"YOUR_FIRST_NAME\" # Use proper capitalization\n",
    "last_name = \"YOUR_LAST_NAME\" # Use proper capitalization\n",
    "\n",
    "hw1_answers.save_to_json(assignment_name, first_name, last_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions?\n",
    "\n",
    "Reach out to your instructors on Piazza.\n",
    "\n",
    "## Sources\n",
    "\n",
    "***\n",
    "\n",
    "<sup>[1]</sup> Ng, A. Stanford University, CS229 Notes: Reinforcement Learning and Control.\n",
    "\n",
    "<sup>[2]</sup> Barnab√°s P√≥czos, Carnegie Mellon, Introduction To Machine Learning: Reinforcement Learning (Course).\n",
    "\n",
    "<sup>[3]</sup> Sutton, R. S., Barto, A. G. (2018 ). Reinforcement Learning: An Introduction. The MIT Press. \n",
    "\n",
    "<sup>[4]</sup> OpenAI: Spinning Up. Retrieved from https://spinningup.openai.com/en/latest/spinningup/rl_intro.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
