{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"skoltech_logo.png\" alt=\"Skoltech\" width=80% height=60% style=\"padding-right:80px;\"/>\n",
    "<h1 style=\"color:#333333; text-align:center; line-height: 0;\">Reinforcement Learning</h1>\n",
    "<h5 style=\"color:#333333; text-align:center;\">Course MA030422</h5>\n",
    "\n",
    "<h2 style=\"color:#A7BD3F;\">Homework 3</h2>\n",
    "\n",
    "***\n",
    "\n",
    "In the previous homework, you learned about linear approximation in RL and how to apply it to continuous state and action space environments, specifically ENDI from Rcognita. In particular, you implemented a *Trajectory-Based Value Iteration* algorithm that utilized least-squares optimization to find the optimal policy and Q functions.\n",
    "\n",
    "### Goal of this homework\n",
    "\n",
    "In this homework, we will be expanding our understanding of approximation methods for RL by studying different approaches to estimating the optimal policy and value-functions for the ENDI environment. In particular, we will examine and implement 5 different algorithms, all of which are based on Monte-Carlo prediction.\n",
    "\n",
    "### Components\n",
    "\n",
    "* Relevant concepts \n",
    "* Exericse 1 - Monte-Carlo methods\n",
    "    * Problem 1.1 - 10 points\n",
    "* Exericse 2 - Neural Networks with MC\n",
    "    * Problem 2.1 - 5 points\n",
    "    * Problem 2.2 - 5 points\n",
    "* Exericse 3 - TD Learning for Q-function optimization\n",
    "    * Problem 3.1 - 10 points\n",
    "    * Problem 3.2 - 5 points\n",
    "\n",
    "Total points: 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\">Imports and Autograder</h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rcognita import EndiSystem, EndiControllerBase, Simulation, AnswerTracker\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# misc\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from IPython.display import HTML, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading</font>\n",
    "Run this cell to track your answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADING DO NOT MODIFY\n",
    "hw3_answers = AnswerTracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\">Relevant concepts</h2>\n",
    "\n",
    "Let's refresh on the relevant concepts covered in the ensuing sections:\n",
    "\n",
    "### Off-line learning v.s. On-line learning\n",
    "\n",
    "Off-line:\n",
    "* Offline RL methods are applicable if data can be obtained in advance and in bulk.\n",
    "* This means that the dataset is acquired in a prior phase (to the training phase of the model). \n",
    "    * In other words: model training happens separately from model evaluation.\n",
    "\n",
    "On-line:\n",
    "* Online learning is an approach that ingests data one observation at a time, as the data comes in from the environment.\n",
    "* Online RL algorithms learn a solution by interacting with the system, and can therefore be applied even when data is not available in advance.\n",
    "* In other words, training and evaluation happen sequentially in a single simulation\n",
    "\n",
    "\n",
    "### On-policy learning v.s. off-policy learning\n",
    "\n",
    "On-policy:\n",
    "* In on-policy learning, we 1) start with a single policy 2) use it to determine next actions and states, and 3) at each time step (interaction with the environment), we optimize this policy to improve it. Usually, policy optimization involves stochastic methods for exploration, either in action sampling or introducing disturbance into the environment.\n",
    "\n",
    "Off-policy:\n",
    "* Off-policy entails the use two policies, whereby one policy (the \"behaviour\" policy) is used for taking steps and collecting samples in order to optimize the other (\"target\") policy. Usually, in this case the behaviour policy is used to enhance how exploration is done, and its key purpose is to collect samples to improve the target policy.\n",
    "\n",
    "In Sutton's words, page 110 <sup>[1]</sup>:\n",
    "> Recall that the distinguishing feature of on-policy methods is that they estimate the value of a policy while using it for control. In off-policy methods these two functions are separated. The policy used to generate behavior, called the behavior policy, may in fact be unrelated to the policy that is evaluated and improved, called the target policy. An advantage of this separation is that the target policy may be deterministic (e.g., greedy), while the behavior policy can continue to sample all possible actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\">Exercise 1 - Monte-Carlo methods</h2>\n",
    "\n",
    "***\n",
    "\n",
    "In chapter 5 of the class text<sup>[1]</sup>, Sutton describes Monte Carlo (MC) methods-- \"used for estimating value functions and discovering optimal policies\". One of the benefits of MC methods comes from the fact that they do not require knowledge of the environment:\n",
    "> Monte Carlo methods require only experience‚Äîsample sequences of states, actions, and rewards from actual or simulated interaction with an environment. Learning from actual experience is striking because it requires no prior knowledge of the environment‚Äôs dynamics, yet can still attain optimal behavior.\n",
    "\n",
    "Sutton clarifies that the MC methods presented in chapter 5 \"[are used for solving] reinforcement learning problem based on **averaging sample returns**\". Note that these methods are:\n",
    "> * Only incremental in an episode-by-episode sense, **but not in a step-by-step (online) sense.**\n",
    "> * Meaning, only on the completion of an episode are value estimates and policies changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Problem 1.1 - Algorithm 1</font>\n",
    "\n",
    "In this problem we will be implementing a MC method for policy estimation that operates differently from Sutton's description above. Its difference lies in that it does not \"average returns for each state‚Äìaction pair\" after each episode, but instead sums returns for each state-action pair over a trajectory for each time step of an episode, using randomly generated actions.\n",
    "\n",
    "<img src=\"MCTBVI.png\" width=70% height=70% />\n",
    "\n",
    "This algorithm is denoted as **MCTBVI**. To reiterate, it is:\n",
    "* MC-based prediction\n",
    "* On-line\n",
    "* On-Policy (although for the purposes of simplicity, we don't update the value function).\n",
    "\n",
    "#### üéØ Task 1 - implement the algorithm above in the code below. Hints:\n",
    "* Work method by method\n",
    "* Fill in the code designated by comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTBVI(EndiControllerBase):\n",
    "    def __init__(self, system, horizon_length=10, n_actions=10, **kwargs):\n",
    "        super(MCTBVI, self).__init__(system, **kwargs)\n",
    "        self.ctrl_mode = 3\n",
    "        self.gamma = 0.95\n",
    "        self.x_buffer = self.y_buffer\n",
    "        self.horizon_length = horizon_length\n",
    "        self.initial_state = system.system_state\n",
    "        self.control_bounds = system.control_bounds\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def _sample_actions(self, u_curr, n_actions):\n",
    "        \"\"\" Sample random actions\n",
    "\n",
    "            Description: given an current action `u_curr`, samples `n_actions` random actions adds them to the current action. Then clips action matrix to within the bounds specified by `control_bounds`.\n",
    "\n",
    "            Args:\n",
    "                u_curr : int\n",
    "                    * current action/control\n",
    "\n",
    "                n_actions : int\n",
    "                    * number of actions to sample\n",
    "\n",
    "            Returns:\n",
    "                u_tile : float matrix\n",
    "                    * matrix of actions\n",
    "\n",
    "        \"\"\"\n",
    "        u_tile = np.tile(u_curr, (n_actions, 1))\n",
    "        \n",
    "        for i in range(n_actions):\n",
    "            # sample force from random uniform distribution (hint: use np.random.uniform)\n",
    "            f = np.random.uniform()\n",
    "            \n",
    "            # sample turning torque (m) from random uniform distribution (hint: use np.random.uniform)\n",
    "            m = np.random.uniform()\n",
    "            change_in_u = np.array((f,m))\n",
    "            u_tile[i] = u_tile[i] + change_in_u\n",
    "            \n",
    "            for j, value in enumerate(u_tile[i]):\n",
    "                u_tile[i,j] = np.clip(value, self.control_bounds[j, 0], self.control_bounds[j, 1])\n",
    "            print('u_tile.shape',u_tile.shape)\n",
    "        return u_tile\n",
    "\n",
    "    def _policy_mc(self, x_input):\n",
    "        \"\"\" Monte Carlo Policy\n",
    "\n",
    "            Description: returns an action given a state\n",
    "\n",
    "            Args:\n",
    "                x_input : 5-d float vector\n",
    "                    * state\n",
    "\n",
    "            Returns:\n",
    "                best_action : 2-d float vector\n",
    "                    * action/control\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Line 4: sample 10 actions\n",
    "        sampled_u = self._sample_actions(self.u_curr, self.n_actions)\n",
    "        print('sampled_u',sampled_u.shape)\n",
    "        # create empty np container to store action values\n",
    "        action_values = np.empty(sampled_u.shape)\n",
    "\n",
    "        # Line 5: loop through every action\n",
    "        for i, u in enumerate(sampled_u):\n",
    "            print('u',u)\n",
    "            # tile the action across a horizon (hint: use np.tile)\n",
    "            u_trajectory = np.tile(u, self.horizon_length)\n",
    "            print('u_trajectory',u_trajectory)\n",
    "\n",
    "            # create empty np container to store states for each step of an action\n",
    "            x_trajectory = np.empty([u_trajectory.shape[0], x_input.shape[0]])\n",
    "            #print('x_trajectory.shape',x_trajectory.shape)\n",
    "            # set the starting state\n",
    "            x_trajectory[0, :] = x_input\n",
    "            x = x_input\n",
    "\n",
    "            # take steps with the current action\n",
    "            for k in range(1, self.horizon_length):\n",
    "                x = x + self.step_size * self.sys_dynamics(None, x, u, self.m, self.I, self.dim_state, self.is_disturb)\n",
    "\n",
    "                x_trajectory[k, :] = x\n",
    "\n",
    "            # Line 6: calculate discounted return (hint: use _value_function)\n",
    "            discounted_return = self._value_function(sampled_u, x_trajectory, self.horizon_length)\n",
    "            \n",
    "            # assign discounted return to current action in `action_values`\n",
    "            action_values[i] = discounted_return\n",
    "            print('i:', i, 'action_values[i]',action_values[i]) \n",
    "        # Line 8: select best action id (hint: use np.argmin)\n",
    "        idx = np.argmin(action_values)\n",
    "\n",
    "        # Line 8: select best action\n",
    "        best_action = sampled_u[idx]\n",
    "\n",
    "        return best_action\n",
    "\n",
    "    def _value_function(self, u_container, x_container, length):\n",
    "        \"\"\" Calculate discounted return over a horizon\n",
    "\n",
    "            Args:\n",
    "                u_container : float matrix\n",
    "                    * matrix of actions\n",
    "\n",
    "                x_container : float matrix\n",
    "                    * matrix of states\n",
    "\n",
    "                length : int\n",
    "                    * iteration end interval\n",
    "\n",
    "            Returns:\n",
    "                J : float\n",
    "                    * sum of discounted return\n",
    "\n",
    "        \"\"\"\n",
    "        J = 0\n",
    "\n",
    "        for k in range(1, length):\n",
    "            x = x_container[k - 1, :]\n",
    "            u = u_container[k - 1, :]\n",
    "            x_next = x_container[k, :]\n",
    "            u_next = u_container[k, :]\n",
    "\n",
    "            J += self.gamma**k * self.running_cost(x, u)\n",
    "\n",
    "        return J\n",
    "\n",
    "    def compute_action(self, t, x):\n",
    "        \"\"\" Compute next action\n",
    "\n",
    "            Description: called by simulation to take steps through the env\n",
    "\n",
    "            Args:\n",
    "                t : float\n",
    "                    * time\n",
    "\n",
    "                x : 5-d float vector\n",
    "                    * state\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        time_since_last_action = t - self.ctrl_clock\n",
    "\n",
    "        if time_since_last_action >= self.sample_time:\n",
    "            # select current action given state x (hint: call _policy_mc on current state x)\n",
    "            self.u_curr = self._policy_mc(x)\n",
    "\n",
    "            return self.u_curr\n",
    "\n",
    "        else:\n",
    "            return self.u_curr\n",
    "\n",
    "    def running_cost(self, x, u):\n",
    "        \"\"\" Cost function aka reward function aka utility function\n",
    "\n",
    "            Args:\n",
    "                x : 5-d float vector\n",
    "                    * state\n",
    "\n",
    "                u : 2-d float vector\n",
    "                    * control\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        r = (x @ self.Q @ x) + (u @ self.R @ u)\n",
    "\n",
    "        return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Running - run 1...\n",
      "u_tile.shape (10, 2)\n",
      "u_tile.shape (10, 2)\n",
      "u_tile.shape (10, 2)\n",
      "u_tile.shape (10, 2)\n",
      "u_tile.shape (10, 2)\n",
      "u_tile.shape (10, 2)\n",
      "u_tile.shape (10, 2)\n",
      "u_tile.shape (10, 2)\n",
      "u_tile.shape (10, 2)\n",
      "u_tile.shape (10, 2)\n",
      "sampled_u (10, 2)\n",
      "u [0.29124748 0.43550809]\n",
      "u_trajectory [0.29124748 0.43550809 0.29124748 0.43550809 0.29124748 0.43550809\n",
      " 0.29124748 0.43550809 0.29124748 0.43550809 0.29124748 0.43550809\n",
      " 0.29124748 0.43550809 0.29124748 0.43550809 0.29124748 0.43550809\n",
      " 0.29124748 0.43550809]\n",
      "i: 0 action_values[i] [3533.01433312 3533.01433312]\n",
      "u [0.35617234 0.80476104]\n",
      "u_trajectory [0.35617234 0.80476104 0.35617234 0.80476104 0.35617234 0.80476104\n",
      " 0.35617234 0.80476104 0.35617234 0.80476104 0.35617234 0.80476104\n",
      " 0.35617234 0.80476104 0.35617234 0.80476104 0.35617234 0.80476104\n",
      " 0.35617234 0.80476104]\n",
      "i: 1 action_values[i] [3541.25140739 3541.25140739]\n",
      "u [0.00689831 0.1792056 ]\n",
      "u_trajectory [0.00689831 0.1792056  0.00689831 0.1792056  0.00689831 0.1792056\n",
      " 0.00689831 0.1792056  0.00689831 0.1792056  0.00689831 0.1792056\n",
      " 0.00689831 0.1792056  0.00689831 0.1792056  0.00689831 0.1792056\n",
      " 0.00689831 0.1792056 ]\n",
      "i: 2 action_values[i] [3515.89747205 3515.89747205]\n",
      "u [-0.06365207  0.17834089]\n",
      "u_trajectory [-0.06365207  0.17834089 -0.06365207  0.17834089 -0.06365207  0.17834089\n",
      " -0.06365207  0.17834089 -0.06365207  0.17834089 -0.06365207  0.17834089\n",
      " -0.06365207  0.17834089 -0.06365207  0.17834089 -0.06365207  0.17834089\n",
      " -0.06365207  0.17834089]\n",
      "i: 3 action_values[i] [3512.52410301 3512.52410301]\n",
      "u [0.49884219 0.05902859]\n",
      "u_trajectory [0.49884219 0.05902859 0.49884219 0.05902859 0.49884219 0.05902859\n",
      " 0.49884219 0.05902859 0.49884219 0.05902859 0.49884219 0.05902859\n",
      " 0.49884219 0.05902859 0.49884219 0.05902859 0.49884219 0.05902859\n",
      " 0.49884219 0.05902859]\n",
      "i: 4 action_values[i] [3539.01265873 3539.01265873]\n",
      "u [-0.01281222 -0.0864917 ]\n",
      "u_trajectory [-0.01281222 -0.0864917  -0.01281222 -0.0864917  -0.01281222 -0.0864917\n",
      " -0.01281222 -0.0864917  -0.01281222 -0.0864917  -0.01281222 -0.0864917\n",
      " -0.01281222 -0.0864917  -0.01281222 -0.0864917  -0.01281222 -0.0864917\n",
      " -0.01281222 -0.0864917 ]\n",
      "i: 5 action_values[i] [3509.45046656 3509.45046656]\n",
      "u [0.17759887 0.23990428]\n",
      "u_trajectory [0.17759887 0.23990428 0.17759887 0.23990428 0.17759887 0.23990428\n",
      " 0.17759887 0.23990428 0.17759887 0.23990428 0.17759887 0.23990428\n",
      " 0.17759887 0.23990428 0.17759887 0.23990428 0.17759887 0.23990428\n",
      " 0.17759887 0.23990428]\n",
      "i: 6 action_values[i] [3525.06519609 3525.06519609]\n",
      "u [-0.38757019  0.82332659]\n",
      "u_trajectory [-0.38757019  0.82332659 -0.38757019  0.82332659 -0.38757019  0.82332659\n",
      " -0.38757019  0.82332659 -0.38757019  0.82332659 -0.38757019  0.82332659\n",
      " -0.38757019  0.82332659 -0.38757019  0.82332659 -0.38757019  0.82332659\n",
      " -0.38757019  0.82332659]\n",
      "i: 7 action_values[i] [3526.71953077 3526.71953077]\n",
      "u [-0.45741882 -0.04671704]\n",
      "u_trajectory [-0.45741882 -0.04671704 -0.45741882 -0.04671704 -0.45741882 -0.04671704\n",
      " -0.45741882 -0.04671704 -0.45741882 -0.04671704 -0.45741882 -0.04671704\n",
      " -0.45741882 -0.04671704 -0.45741882 -0.04671704 -0.45741882 -0.04671704\n",
      " -0.45741882 -0.04671704]\n",
      "i: 8 action_values[i] [3485.82953522 3485.82953522]\n",
      "u [-0.40276857  0.58154451]\n",
      "u_trajectory [-0.40276857  0.58154451 -0.40276857  0.58154451 -0.40276857  0.58154451\n",
      " -0.40276857  0.58154451 -0.40276857  0.58154451 -0.40276857  0.58154451\n",
      " -0.40276857  0.58154451 -0.40276857  0.58154451 -0.40276857  0.58154451\n",
      " -0.40276857  0.58154451]\n",
      "i: 9 action_values[i] [3514.21213855 3514.21213855]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 16 is out of bounds for axis 0 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-088ab6a7faa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mexit_py_on_finish\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0;31m#show_summary_stats=True,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                 print_statistics_at_step=False)\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/rcognita/lib/python3.7/site-packages/rcognita/simulation.py\u001b[0m in \u001b[0;36mrun_simulation\u001b[0;34m(self, n_runs, is_visualization, fig_width, fig_height, exit_py_on_finish, show_annotations, print_summary_stats, print_statistics_at_step, print_inline, is_log_data)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapper_take_steps_no_viz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontroller\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/rcognita/lib/python3.7/site-packages/rcognita/simulation.py\u001b[0m in \u001b[0;36m_wrapper_take_steps_no_viz\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1126\u001b[0;31m                 \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/rcognita/lib/python3.7/site-packages/rcognita/simulation.py\u001b[0m in \u001b[0;36m_take_step\u001b[0;34m(self, system, controller, simulator, animate, mid)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0mcontroller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_sys_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontroller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_latest_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-560a67a56aaf>\u001b[0m in \u001b[0;36mcompute_action\u001b[0;34m(self, t, x)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtime_since_last_action\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_time\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;31m# select current action given state x (hint: call _policy_mc on current state x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy_mc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu_curr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-560a67a56aaf>\u001b[0m in \u001b[0;36m_policy_mc\u001b[0;34m(self, x_input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# Line 8: select best action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mbest_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampled_u\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbest_action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 16 is out of bounds for axis 0 with size 10"
     ]
    }
   ],
   "source": [
    "sys = EndiSystem()\n",
    "agent1 = MCTBVI(sys, sample_time=0.3, horizon_length=10, t1=30)\n",
    "sim = Simulation(sys, agent1)\n",
    "sim.run_simulation(n_runs=2, \n",
    "                is_visualization=False, \n",
    "                exit_py_on_finish=False, \n",
    "                #show_summary_stats=True, \n",
    "                print_statistics_at_step=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading</font>\n",
    "Run this cell to track your answers and to save your answer for problem 1.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADING DO NOT MODIFY\n",
    "statistics, = sim.final_statistics\n",
    "\n",
    "hw3_answers.record('problem_1-1', \n",
    "    {'mean_rc': statistics[0],\n",
    "    'mean_velocity': statistics[1], \n",
    "    'sd_rc': statistics[2],\n",
    "    'sd_velocity': statistics[3],\n",
    "    'sd_alpha': statistics[4],\n",
    "    'final_l2_norm': statistics[5],\n",
    "    'sd_l2_norm': statistics[6],\n",
    "    'rmse_l2_norm': statistics[7]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\">Exercise 2 - Neural Networks with MC</h2>\n",
    "\n",
    "MC prediction methods can be combined with deep learning by serving as trainers (aka supervisors) of neural networks. In this exercise, we'll review such implementions in both *off-line* and *on-line* learning cases.\n",
    "\n",
    "### <font color=\"blue\">Problem 2.1 - Algorithm 2</font>\n",
    "\n",
    "Firstly, let's examine how the MC policy from problem 1.1 can be used as a supervisor for a neural network-based policy in the case of an <font color=\"8F00FF\">**off-line**</font> and <font color=\"8F00FF\">**off-policy**</font> method.\n",
    "\n",
    "This algorithm consists of two parts:\n",
    "* The trainer agent\n",
    "* The actor agent\n",
    "\n",
    "#### How does it work\n",
    "\n",
    "The purpose of the trainer agent is solely to train the NN with an MC policy. It does so in two steps:\n",
    "1. Plays out a full episode taking steps with an MC policy. It saves all of its actions and states into buffers.\n",
    "2. Then, it trains the NN with the actions and states stored in the buffers.\n",
    "\n",
    "Afterwards the actor agent performs step 3:\n",
    "3. It uses the trained NN policy to play an episode\n",
    "\n",
    "There are two important takeaways as far as RL taxonomy goes:\n",
    "* Because the training (steps 1 and 2) happens separately from model evaluation (step 3), this algorithm is an example of <font color=\"8F00FF\">**off-line learning**</font>.\n",
    "* Because the NN policy is not concurrently optimized **and** used for step-taking during the simulation, this is an example of an <font color=\"8F00FF\">**off-policy** algorithm</font>.\n",
    "\n",
    "#### üéØ Task 1 - implement the NN trainer in the code below\n",
    "\n",
    "Hints:\n",
    "* Work method by method\n",
    "* Fill in the code designated by comments\n",
    "\n",
    "### <font color=\"#ba03fc\">Step 1</font>\n",
    "\n",
    "#### First, define neural network (for policy approximation)\n",
    "* This is done for you, there's nothing to do in this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNN(nn.Module):\n",
    "    def __init__(self, obs_size=5, hidden_size=128, n_actions=2):\n",
    "        super(PolicyNN, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(obs_size, hidden_size)\n",
    "        self.linear2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = torch.nn.Linear(hidden_size, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = torch.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, inherit MCTBVI class and create trainer agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OfflineTrainerAgent(MCTBVI):\n",
    "    def __init__(self, system, horizon_length=10, n_epochs=1000, **kwargs):\n",
    "        super(OfflineTrainerAgent, self).__init__(system, **kwargs)\n",
    "        self.save_path = os.getcwd() + \"/policy.pth\"\n",
    "\n",
    "        self.policy_nn = PolicyNN()\n",
    "        self.criterion = torch.nn.SmoothL1Loss()\n",
    "        self.optimizer = optim.Adam(params=self.policy_nn.parameters(), lr=0.0005)\n",
    "        \n",
    "        self.u_trajectory = []\n",
    "        self.x_trajectory = []\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "    def train_network(self, overwrite_pth=True, print_last_batch=False):\n",
    "        \"\"\" Train neural network from buffers\n",
    "\n",
    "            Args:\n",
    "                overwrite_pth : boolean\n",
    "                    * overwrite model weights file (titled policy.pth)?\n",
    "\n",
    "                print_last_batch : boolean\n",
    "                    * for debugging, do you want to see how the preds and targets line up for the last training batch?\n",
    "\n",
    "            Returns:\n",
    "                None \n",
    "\n",
    "        \"\"\"\n",
    "        if overwrite_pth is True or not os.path.exists(self.save_path):\n",
    "            print(\"Training neural net...\")\n",
    "\n",
    "            try:\n",
    "                # stack trajectories into np form\n",
    "                u = np.vstack(self.u_trajectory)\n",
    "                x = np.vstack(self.x_trajectory)\n",
    "                dataset = torch.as_tensor(np.concatenate((x,u), axis=1), dtype=torch.float32)\n",
    "            \n",
    "            except:\n",
    "                print(\"Buffers not filled because simulation was not run before training.\")\n",
    "                sys.exit()\n",
    "\n",
    "            # create dataloader for pytorch\n",
    "            self.trainloader = torch.utils.data.DataLoader(dataset, 256, shuffle=False)\n",
    "\n",
    "            for epoch in tqdm(range(self.n_epochs)):\n",
    "                running_loss = 0.0\n",
    "                \n",
    "                # for in trainloader\n",
    "                for batch in self.trainloader:\n",
    "                    # zero out gradients in optimizer\n",
    "                    ### YOUR CODE HERE\n",
    "\n",
    "                    # specify states of batch (hint: first 5 column vectors of batch)\n",
    "                    states = ### YOUR CODE HERE\n",
    "\n",
    "                    # specify target actions of batch (hint: last 2 column vectors of batch)\n",
    "                    target_actions = ### YOUR CODE HERE\n",
    "\n",
    "                    # predict actions for batch using policy network\n",
    "                    predicted_action = ### YOUR CODE HERE\n",
    "\n",
    "                    # calculate loss\n",
    "                    self.loss = ### YOUR CODE HERE\n",
    "\n",
    "                    # calculate gradients of loss w.r.t all parameters in computational graph\n",
    "                    ### YOUR CODE HERE\n",
    "\n",
    "                    # update weights of neural network\n",
    "                    ### YOUR CODE HERE\n",
    "\n",
    "                    # increment running loss\n",
    "                    running_loss += self.loss\n",
    "\n",
    "            if print_last_batch is True:\n",
    "                print(torch.cat((predicted_action, target_actions), dim=1))\n",
    "\n",
    "            torch.save(self.policy_nn.state_dict(), self.save_path)\n",
    "            print(\"...Policy saved.\")\n",
    "\n",
    "        else:\n",
    "            print(\"Skipping training, policy file exists and overwrite is False.\")\n",
    "    \n",
    "    def compute_action(self, t, x):\n",
    "        time_since_last_action = t - self.ctrl_clock\n",
    "\n",
    "        if time_since_last_action >= self.sample_time:\n",
    "            \n",
    "            self.u_curr = self._policy_mc(x)\n",
    "\n",
    "            # fill buffers for training\n",
    "            self.u_trajectory.append(self.u_curr)\n",
    "            self.x_trajectory.append(x)\n",
    "\n",
    "            return self.u_curr\n",
    "\n",
    "        else:\n",
    "            return self.u_curr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run trainer agent simulation to fill buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys = EndiSystem()\n",
    "trainer = OfflineTrainerAgent(sys, sample_time=0.3, horizon_length=10, t1=30)\n",
    "sim = Simulation(sys, trainer)\n",
    "sim.run_simulation(n_runs=1, \n",
    "                is_visualization=False, \n",
    "                exit_py_on_finish=False,  \n",
    "                show_summary_stats=True, \n",
    "                print_statistics_at_step=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to visualize\n",
    "# %matplotlib notebook\n",
    "# HTML(sim.run_simulation(n_runs=2, fig_width=7, fig_height=7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#ba03fc\">Step 2 - </font> Run NN training \n",
    "\n",
    "Note that this operation saves the NN weights to a file called `policy.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#ba03fc\">Step 3</font>\n",
    "\n",
    "#### Finally, implement the NN actor agent in the code below\n",
    "* This *actor* will play an episode using the NN as the policy function.\n",
    "* There's nothing for you to do on this one, just execute the code and make sure you understand why it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OfflineActorAgent(EndiControllerBase):\n",
    "    def __init__(self, system, **kwargs):\n",
    "        super(OfflineActorAgent, self).__init__(system, **kwargs)\n",
    "        self.ctrl_mode = 3\n",
    "\n",
    "        self.policy_nn = PolicyNN()\n",
    "        self.save_path = os.getcwd() + \"/policy.pth\"\n",
    "\n",
    "        try:\n",
    "            # load model weights from policy.pth\n",
    "            self.policy_nn.load_state_dict(torch.load(self.save_path))\n",
    "\n",
    "            # set the NN to evaluation mode\n",
    "            self.policy_nn.eval()\n",
    "            print(\"Policy loaded.\")\n",
    "        except:\n",
    "            print(\"Failed to load policy.\")\n",
    "\n",
    "\n",
    "    def compute_action(self, t, x):\n",
    "        time_since_last_action = t - self.ctrl_clock\n",
    "\n",
    "        if time_since_last_action >= self.sample_time:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                x = torch.FloatTensor(x)\n",
    "                self.u_curr = self.policy_nn(x).detach().numpy()\n",
    "\n",
    "            return self.u_curr\n",
    "\n",
    "        else:\n",
    "            return self.u_curr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now run the actor agent to see how the NN policy performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sys = EndiSystem()\n",
    "agent2 = OfflineActorAgent(sys, sample_time=0.3, t1=30)\n",
    "sim = Simulation(sys, agent2)\n",
    "sim.run_simulation(n_runs=2, \n",
    "                is_visualization=False, \n",
    "                exit_py_on_finish=False,  \n",
    "                show_summary_stats=True, \n",
    "                print_statistics_at_step=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to visualize\n",
    "# %matplotlib notebook\n",
    "# HTML(sim.run_simulation(n_runs=2, fig_width=7, fig_height=7, show_annotations=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "The trainer agent played 2 episodes, recorded $x$ and $u$ into its buffers, and then trained the NN policy to learn the best (lowest cost) $u$ for a given $x$. If you compare the final L2-norm and standard deviations of the L2-norms for both trainer and actor, you'll see they are very close. \n",
    "\n",
    "In conclusion, the simulation of the actor shows us that:\n",
    "* a) the MC policy has been memorized by the NN\n",
    "* b) quickly converges to (0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading</font>\n",
    "Run this cell to track your answers and to save your answer for problem 2.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADING DO NOT MODIFY\n",
    "statistics, = sim.final_statistics\n",
    "\n",
    "hw3_answers.record('problem_2-1', \n",
    "    {'mean_rc': statistics[0],\n",
    "    'mean_velocity': statistics[1], \n",
    "    'sd_rc': statistics[2],\n",
    "    'sd_velocity': statistics[3],\n",
    "    'sd_alpha': statistics[4],\n",
    "    'final_l2_norm': statistics[5],\n",
    "    'sd_l2_norm': statistics[6],\n",
    "    'rmse_l2_norm': statistics[7]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Problem 2.2 - Algorithm 3</font>\n",
    "\n",
    "In this problem we will review an <font color=\"8F00FF\">**on-line**</font> and <font color=\"8F00FF\">**on-policy**</font> version of the same algorithm above. \n",
    "\n",
    "#### üéØ Task 1 - implement the algorithm below in the lines specified by the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNOnlineMC(MCTBVI):\n",
    "    def __init__(self, system, horizon_length=10, num_iter = 30, **kwargs):\n",
    "        super(NNOnlineMC, self).__init__(system, **kwargs)\n",
    "        self.ctrl_mode = 3\n",
    "        self.horizon_length = horizon_length\n",
    "        self.control_bounds = system.control_bounds\n",
    "        self.gamma = 0.95\n",
    "        self.num_iter = num_iter\n",
    "\n",
    "        self.save_path = os.getcwd() + \"/policy_online.pth\"\n",
    "\n",
    "        self.policy_nn = PolicyNN()\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(params=self.policy_nn.parameters(), lr=0.0005)\n",
    "    \n",
    "    def _policy_iteration(self, x, u_target):\n",
    "        xt = torch.tensor(x, dtype=torch.float32)\n",
    "        u_target = torch.tensor(u_target, dtype=torch.float32)\n",
    "\n",
    "        # loop through a number of training iterations\n",
    "        for i in range(self.num_iter):\n",
    "            # policy evaluation step\n",
    "            # calculate u_pred\n",
    "            u_pred = ### YOUR CODE HERE\n",
    "\n",
    "            # calcuclate loss between u_pred and u_target\n",
    "            loss = ### YOUR CODE HERE\n",
    "\n",
    "            # policy improvement\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        return u_pred.detach().numpy()\n",
    "\n",
    "    def compute_action(self, t, x):\n",
    "        time_since_last_action = t - self.ctrl_clock\n",
    "\n",
    "        if time_since_last_action >= self.sample_time:\n",
    "            u_target = self._policy_mc(x)\n",
    "            self.u_curr = self._policy_iteration(x, u_target)\n",
    "\n",
    "            return self.u_curr\n",
    "\n",
    "        else:\n",
    "            return self.u_curr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys = EndiSystem()\n",
    "agent3 = NNOnlineMC(sys, sample_time=0.3, horizon_length=10, num_iter=30, t1=22)\n",
    "sim = Simulation(sys, agent3)\n",
    "sim.run_simulation(n_runs=3, \n",
    "                is_visualization=False, \n",
    "                exit_py_on_finish=False,  \n",
    "                show_summary_stats=True, \n",
    "                print_statistics_at_step=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to visualize\n",
    "# %matplotlib notebook\n",
    "# HTML(sim.run_simulation(n_runs=2, fig_width=7, fig_height=7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading</font>\n",
    "Run this cell to track your answers and to save your answer for problem 1.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADING DO NOT MODIFY\n",
    "statistics, = sim.final_statistics\n",
    "\n",
    "hw3_answers.record('problem_2-2', \n",
    "    {'mean_rc': statistics[0],\n",
    "    'mean_velocity': statistics[1], \n",
    "    'sd_rc': statistics[2],\n",
    "    'sd_velocity': statistics[3],\n",
    "    'sd_alpha': statistics[4],\n",
    "    'final_l2_norm': statistics[5],\n",
    "    'sd_l2_norm': statistics[6],\n",
    "    'rmse_l2_norm': statistics[7]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\">Exercise 3 - TD Learning</h2>\n",
    "\n",
    "***\n",
    "Let's start this exercise by reviewing temporal difference learning from chapter 6 in the class text. What is the difference between TD and MC? <sup>[1]</sup>:\n",
    "> Both TD and Monte Carlo methods use experience to solve the prediction problem. Given some experience following a policy $\\pi$, both methods update their estimate $v_{\\pi}$ for the nonterminal states $S_t$ occurring in that experience. Roughly speaking, Monte Carlo methods wait until the **return** following the visit is known, then use that return as a target for $V(S_t)$.\n",
    "\n",
    "\n",
    "Sutton continues on page 120 <sup>[1]</sup>:\n",
    "<blockquote>\n",
    "Whereas Monte Carlo methods must wait until the end of the episode to determine the increment to $V(S_t)$ (only then is $G_t$ known), TD methods need to wait only until the next time step. At time $t + 1$ they immediately form a target and make a useful update using the observed reward $R_{t+1}$ and the estimate $V(S_{t+1})$.\n",
    "\n",
    "The simplest TD method makes the update: \n",
    "$$V\\left(S_{t}\\right) \\leftarrow V\\left(S_{t}\\right)+\\alpha\\left[R_{t+1}+\\gamma V\\left(S_{t+1}\\right)-V\\left(S_{t}\\right)\\right]$$\n",
    "\n",
    "immediately on transition to $S_{t+1}$ and receiving $R_{t+1}$. In effect, the target for the Monte Carlo update is $G_t$, whereas the target for the TD update is $R_{t+1}$ + $V(S_{t+1})$. This TD method is called TD(0), or one-step TD, because it is a special case of the $TD(\\cdot)$ and n-step TD methods developed in Chapter 12 and Chapter 7. The box below specifies TD(0) completely in procedural form.\n",
    "\n",
    "<img src=\"sutton_120.png\" width=70% height=70% />\n",
    "    \n",
    "Roughly speaking, Monte Carlo methods use an estimate of (6.3) as a target, whereas DP methods use an estimate of (6.4) as a target. The Monte Carlo target is an estimate because the expected value in (6.3) is not known; a sample return is used in place of the real expected return. The DP target is an estimate not because of the expected values, which are assumed to be completely provided by a model of the environment, but because $V(S_{t+1})$ is not known and the current estimate, $V(S_{t+1})$, is used instead. \n",
    "\n",
    "The TD target is an estimate for both reasons: it samples the expected values in (6.4) and it uses the current estimate V instead of the true $v_{\\pi}$. Thus, TD methods combine the sampling of Monte Carlo with the bootstrapping of DP. As we shall see, with care and imagination this can take us a long way toward obtaining the advantages of both Monte Carlo and DP methods.\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Problem 3.1 - Sarsa</font>\n",
    "\n",
    "In the first problem of exercise 3 we will examine Sarsa. It is an <font color=\"8F00FF\">**on-policy**</font> TD method described by Sutton on page 129 <sup>[1]</sup>. Sutton uses the following outline for Sarsa:\n",
    "    \n",
    "<img src=\"sarsa_sutton.png\" width=75% height=75% />\n",
    "\n",
    "‚ùóNote the action-value update in the diagram above. The purpose of this update applies to **discrete state and action-space environments** (aka tabular learning) where action-values $Q(s,a)$ need to be stored, updated, and retrieved to find the optimal policy in each episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚û°Ô∏è In our case, we are applying Sarsa to a **continuous state and action space** env (ENDI) using a NN as an approximator of the Q-function. Therefore, we will be implementing a version of Sarsa based exclusively on minimizing TD, which works for function approximation, gradient updates, and approximate dynamic programming algorithms:\n",
    "\n",
    "<img src=\"SarsaC.png\" width=70% height=70% />\n",
    "\n",
    "Notice that the temporal difference calculation on line 7 ($\\delta$) is identical to that of Sutton's version for finite MDPs. \n",
    "\n",
    "The primary difference between this algorithm and Sutton's is that in our version, Q-values are approximated rather than calculated/updated from tabular lookups. Therefore, we update the **Q-function** rather than Q-values, and we do so by minimizing TD with respect to the weights/parameters of the Q-function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üéØ Task: implement the code specified by the comments. You only need to update:\n",
    "* `_sum_q_over_traj` method\n",
    "* `_temporal_diff` method\n",
    "\n",
    "Importantly:\n",
    "* the other methods should be familiar to you from above\n",
    "* the `policy_mc` method is nearly identical to that seen above - **however** it now implements an epsilon greedy policy.\n",
    "* Notice that we are using a fairly large epsilon: `0.3`\n",
    "* Regarding the Q-function neural net:\n",
    "    * Notice that we pass `self.linear3(x)` to an exponential function to ensure that our output values are always positive, since Q must be >= 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNN(nn.Module):\n",
    "    def __init__(self, obs_size=7, hidden_size=32):\n",
    "        super(QNN, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(obs_size, hidden_size)\n",
    "        self.linear2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = torch.nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.linear1(x))\n",
    "        x = torch.tanh(self.linear2(x))\n",
    "        x = torch.exp(self.linear3(x))\n",
    "        return x\n",
    "\n",
    "class SARSA(EndiControllerBase):\n",
    "    def __init__(self, system, horizon_length=10, epsilon=0.3, **kwargs):\n",
    "        super(SARSA, self).__init__(system, **kwargs)\n",
    "        self.ctrl_mode = 3\n",
    "        self.horizon_length = horizon_length\n",
    "        self.control_bounds = system.control_bounds\n",
    "        self.gamma = 0.95\n",
    "        self.alpha = 0.5\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = 10\n",
    "\n",
    "        self.q_fn = QNN()\n",
    "        self.optimizer = optim.SGD(params=self.q_fn.parameters(), lr=0.005)\n",
    "\n",
    "        R = np.diag([0, 0])\n",
    "        Q = np.diag([10, 10, 1, 0, 0])\n",
    "        self.R_tensor = torch.tensor(R, dtype=torch.float32)\n",
    "        self.Q_tensor = torch.tensor(Q, dtype=torch.float32)\n",
    "\n",
    "    def print_nn_params(self):\n",
    "        print(list(self.q_fn.parameters())[0])\n",
    "    \n",
    "    def _sample_single_action(self):\n",
    "        f = np.random.uniform(-5, 5)\n",
    "        m = np.random.uniform(-1, 1)\n",
    "        action = np.array((f,m))\n",
    "        \n",
    "        for i in range(self.dim_input):\n",
    "            action[i] = np.clip(action[i], self.control_bounds[i, 0], self.control_bounds[i, 1])\n",
    "\n",
    "        return action\n",
    "\n",
    "    def _sample_actions(self, existing_u, n_actions):\n",
    "        u_tile = np.tile(existing_u, (n_actions, 1))\n",
    "        \n",
    "        for i in range(n_actions):\n",
    "            f = np.random.uniform(-5, 5)\n",
    "            m = np.random.uniform(-1, 1)\n",
    "            change_in_u = np.array((f,m))\n",
    "            u_tile[i] = u_tile[i] + change_in_u\n",
    "            \n",
    "            for j, value in enumerate(u_tile[i]):\n",
    "                u_tile[i,j] = np.clip(value, self.control_bounds[j, 0], self.control_bounds[j, 1])\n",
    "\n",
    "        return u_tile\n",
    "    \n",
    "    def _policy_mc(self, x_input):\n",
    "        sampled_u = self._sample_actions(self.u_curr, self.n_actions)\n",
    "        action_values = np.empty(self.n_actions)\n",
    "\n",
    "        for i in range(self.n_actions):\n",
    "            u_trajectory = np.tile(sampled_u[i,:], (self.horizon_length, 1))\n",
    "            x_trajectory = np.zeros([self.horizon_length, self.dim_output])\n",
    "            x_trajectory[0, :] = x_input\n",
    "            x = x_input\n",
    "\n",
    "            for k in range(1, self.horizon_length):\n",
    "                x = x + self.step_size * self.sys_dynamics(None, x, u_trajectory[k - 1, :], self.m, self.I, self.dim_state, self.is_disturb)\n",
    "\n",
    "                x_trajectory[k, :] = x\n",
    "\n",
    "            u_trajectory = torch.as_tensor(u_trajectory).float()\n",
    "            x_trajectory = torch.as_tensor(x_trajectory).float()\n",
    "\n",
    "            q_over_traj = self._sum_q_over_traj(u_trajectory, x_trajectory, self.horizon_length)\n",
    "            action_values[i] = q_over_traj.item()\n",
    "\n",
    "        idx = np.argmin(action_values)\n",
    "        best_action = sampled_u[idx]\n",
    "\n",
    "        # epsilon greedy policy\n",
    "        if torch.rand(1).item() > self.epsilon:\n",
    "            return best_action, None\n",
    "\n",
    "        else:\n",
    "            rand_action = self._sample_single_action()\n",
    "\n",
    "            return rand_action, best_action\n",
    "\n",
    "\n",
    "    def _sum_q_over_traj(self, u_container, x_container, length):\n",
    "        sum_q = 0\n",
    "\n",
    "        for k in range(1, length):\n",
    "            x = x_container[k - 1, :]\n",
    "            u = u_container[k - 1, :]\n",
    "            x_next = x_container[k, :]\n",
    "            u_next = u_container[k, :]\n",
    "            z = torch.cat((x_next, u_next))\n",
    "\n",
    "            # calculate q_value: Q(s,a) = R(x,u) + (gamma^k)*Q(x_t+1, u_t+1)\n",
    "            q_value = ### YOUR CODE HERE\n",
    "            sum_q += q_value\n",
    "\n",
    "        return sum_q\n",
    "\n",
    "    def _take_action(self, x, u):\n",
    "        x_next = x + self.step_size * self.sys_dynamics(None, x, u, self.m, self.I, self.dim_state, self.is_disturb)\n",
    "\n",
    "        return x_next\n",
    "\n",
    "    def _temporal_diff(self, u_current, x_current, u_next, x_next):\n",
    "        u_current = torch.as_tensor(u_current).float()\n",
    "        u_next = torch.as_tensor(u_next).float()\n",
    "        x_current = torch.as_tensor(x_current).float()\n",
    "        x_next = torch.as_tensor(x_next).float()\n",
    "        \n",
    "        running_cost = self.running_cost_tensors(x_current, u_current)\n",
    "        \n",
    "        z_current = torch.cat((x_current, u_current))\n",
    "        q_current = self.q_fn(z_current)\n",
    "        \n",
    "        z_next = torch.cat((x_next, u_next))\n",
    "        q_next = self.q_fn(z_next)\n",
    "\n",
    "        # calculate temporal difference: td = alpha*(running_cost + (gamma*q_next) - q_current)\n",
    "        td = ### YOUR CODE HERE\n",
    "\n",
    "        return td\n",
    "\n",
    "    def compute_action(self, t, x_current):\n",
    "        time_since_last_action = t - self.ctrl_clock\n",
    "\n",
    "        if time_since_last_action >= self.sample_time:\n",
    "            \n",
    "            self.q_fn.eval()\n",
    "\n",
    "            # line 3\n",
    "            u_current = self.u_curr\n",
    "\n",
    "            # line 5\n",
    "            x_next = self._take_action(x_current, u_current)\n",
    "\n",
    "            # line 6\n",
    "            u_next, _ = self._policy_mc(x_next)\n",
    "\n",
    "            # line 7\n",
    "            td = self._temporal_diff(self.u_curr, x_current, u_next, x_next)\n",
    "\n",
    "            # Q-function update starts here\n",
    "            self.q_fn.train()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # line 8\n",
    "            td.backward()\n",
    "            self.optimizer.step()\n",
    "            # Q-function update ends here\n",
    "\n",
    "            # line 9\n",
    "            self.u_curr = u_next\n",
    "\n",
    "            return self.u_curr\n",
    "\n",
    "        else:\n",
    "            return self.u_curr\n",
    "\n",
    "    def running_cost_tensors(self, x, u):\n",
    "        r = (x @ self.Q_tensor @ x) + (u @ self.R_tensor @ u)\n",
    "\n",
    "        return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sys, agent, sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys = EndiSystem()\n",
    "agent4 = SARSA(sys, sample_time=0.3, horizon_length=10, epsilon=0.3, t1=25)\n",
    "sim = Simulation(sys, agent4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run sim (non-visual)\n",
    "* If running multiple times, be sure to re-instantiate the agent above to re-initialize its NN weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.run_simulation(n_runs=3, \n",
    "                is_visualization=False, \n",
    "                exit_py_on_finish=False,  \n",
    "                show_summary_stats=True, \n",
    "                print_statistics_at_step=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to visualize\n",
    "# %matplotlib notebook\n",
    "# HTML(sim.run_simulation(n_runs=3, fig_width=7, fig_height=7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take notice\n",
    "\n",
    "The epsilon $\\epsilon$ hyperparameter dictates the probability that a random action is selected as policy. \n",
    "\n",
    "Because Sarsa is **on-policy** -- then given a) non-zero $\\epsilon$ and b) over-repeated trials, it's Q-function will take more updates/steps to reach optimality than an algorithm that uses the maximum Q-value for the Q-function update, like *Q-learning* (which we review next).\n",
    "\n",
    "As Sutton states:\n",
    "> * The convergence properties of the Sarsa algorithm depend on the nature of the policy‚Äôs dependence on Q. For example, one could use \"-greedy or \"-soft policies. \n",
    "> * Sarsa converges with probability 1 to an optimal policy and action-value function as long as all state‚Äìaction pairs are visited an infinite number of times and the policy converges in the limit to the greedy policy (which can be arranged, for example, with \"-greedy policies by setting \" = 1/t)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading</font>\n",
    "Run this cell to track your answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADING DO NOT MODIFY\n",
    "statistics, = sim.final_statistics\n",
    "\n",
    "hw3_answers.record('problem_3-1', \n",
    "    {'mean_rc': statistics[0],\n",
    "    'mean_velocity': statistics[1], \n",
    "    'sd_rc': statistics[2],\n",
    "    'sd_velocity': statistics[3],\n",
    "    'sd_alpha': statistics[4],\n",
    "    'final_l2_norm': statistics[5],\n",
    "    'sd_l2_norm': statistics[6],\n",
    "    'rmse_l2_norm': statistics[7]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Problem 3.2 - Q-Learning</font>\n",
    "\n",
    "Q-learning is an <font color=\"8F00FF\">**off-policy**</font> TD algorithm that has only a minor (but important) variation compared to Sarsa <sup>[1]</sup>:\n",
    "> In this case, the learned action-value function, Q, directly approximates $q_{*}$, the optimal action-value function, independent of the policy being followed.\n",
    "\n",
    "Note that \"the policy still has an effect in that it determines which state‚Äìaction pairs are visited and updated\", but the Q-value of the next state is always the maximum value over all possible actions. In the **finite** MDP case, the Q-update becomes <sup>[1]</sup>:\n",
    "\n",
    "$$Q\\left(S_{t}, A_{t}\\right) \\leftarrow Q\\left(S_{t}, A_{t}\\right)+\\alpha\\left[R_{t+1}+\\gamma \\max _{a} Q\\left(S_{t+1}, a\\right)-Q\\left(S_{t}, A_{t}\\right)\\right]$$\n",
    "\n",
    "The algorithm outline in the **finite MDP** case is:\n",
    "\n",
    "<img src=\"qlearning_sutton.png\" width=70% height=70% />\n",
    "\n",
    "Similar to Sarsa, the following Q-learning algorithm is adapted to the **continuous, linear approximation** case:\n",
    "\n",
    "<img src=\"QlearningC.png\" width=70% height=70% />\n",
    "\n",
    "#### Execute the code below and compare it to Sarsa\n",
    "* Make sure you're clear on the difference in these algorithms\n",
    "* Note that we're using the epsilon of `0.3`\n",
    "* There's nothing for you to do on this one - just make sure you know how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning(SARSA):\n",
    "    def __init__(self, system, **kwargs):\n",
    "        super(QLearning, self).__init__(system, **kwargs)\n",
    "\n",
    "    def compute_action(self, t, x_current):\n",
    "        time_since_last_action = t - self.ctrl_clock\n",
    "\n",
    "        if time_since_last_action >= self.sample_time:\n",
    "            \n",
    "            self.q_fn.eval()\n",
    "            \n",
    "            # line 3\n",
    "            u_current = self.u_curr\n",
    "\n",
    "            # line 5\n",
    "            x_next = self._take_action(x_current, u_current)\n",
    "            \n",
    "            # line 6\n",
    "            u_next, u_best = self._policy_mc(x_next)\n",
    "\n",
    "            # line 7: step 1\n",
    "            \"\"\"\n",
    "                Remember that the policy_mc is epsilon-greedy. Sometimes it outputs a random action. When this is the case, we still take the maximum value action and use it in the td calculation.\n",
    "            \"\"\"\n",
    "            # if a random action was *not* selected\n",
    "            if u_best is None:\n",
    "                # then `u_next` is best action: use it for td calculation\n",
    "                u_best = u_next\n",
    "\n",
    "            # line 7: step 2\n",
    "            td = self._temporal_diff(u_current, x_current, u_best, x_next)\n",
    "\n",
    "            # Q-function update starts here\n",
    "            self.q_fn.train()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # line 8\n",
    "            td.backward()\n",
    "            self.optimizer.step()\n",
    "            # Q-function update ends here\n",
    "\n",
    "            self.u_curr = u_next\n",
    "\n",
    "            return self.u_curr\n",
    "\n",
    "        else:\n",
    "            return self.u_curr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sys, agent, sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys = EndiSystem()\n",
    "agent5 = QLearning(sys, sample_time=0.3, horizon_length=10, epsilon=0.3, t1=25)\n",
    "sim = Simulation(sys, agent5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run sim (non-visual)\n",
    "* If running multiple times, be sure to re-instantiate the agent above to re-initialize its NN weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.run_simulation(n_runs=3, \n",
    "            is_visualization=False, \n",
    "            exit_py_on_finish=False, \n",
    "            show_annotations=True, \n",
    "            show_summary_stats=True, \n",
    "            print_statistics_at_step=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to visualize\n",
    "# %matplotlib notebook\n",
    "# HTML(sim.run_simulation(n_runs=3, fig_width=7, fig_height=7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "What do you think- over **a large number of simulations**, for a non-zero $\\epsilon$, which algorithm should produce better performance: Sarsa or Q-learning? Even in the limited simulations above, keeping all variables fixed, Sarsa algorithm *tends* to have a bit higher RMSE of Trajectory than Q-learning (though again, this is not guaranteed because of the e-greedy policy). This means that in the sims above, Sarsa tends to have a less linear path towards (0,0) xy-coords. \n",
    "\n",
    "Q-learning is guaranteed to converge to the optimal Q-function over infinite iterations. Per Sutton:\n",
    "> * In this case, the learned action-value function, Q, directly approximates $q_{*}$, the optimal action-value function, independent of the policy being followed. This dramatically simplifies the analysis of the algorithm and enabled early convergence proofs. The policy still has an effect in that it determines which state‚Äìaction pairs are visited and updated. However, all that is required for correct convergence is that all pairs continue to be updated. As we observed in Chapter 5, this is a minimal requirement in the sense that any method guaranteed to find optimal behavior in the general case must require it. \n",
    "> * <font color=\"green\">Under this assumption and a variant of the usual stochastic approximation conditions on the sequence of step-size parameters, Q has been shown to converge with probability 1 to $q_{*}$.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading</font>\n",
    "Run this cell to track your answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADING DO NOT MODIFY\n",
    "statistics, = sim.final_statistics\n",
    "\n",
    "hw3_answers.record('problem_3-2', \n",
    "    {'mean_rc': statistics[0],\n",
    "    'mean_velocity': statistics[1], \n",
    "    'sd_rc': statistics[2],\n",
    "    'sd_velocity': statistics[3],\n",
    "    'sd_alpha': statistics[4],\n",
    "    'final_l2_norm': statistics[5],\n",
    "    'sd_l2_norm': statistics[6],\n",
    "    'rmse_l2_norm': statistics[7]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading: Submit your answers</font>\n",
    "Enter your first and last name in the cell below **precisely as it appears in Canvas**, and then run the cell to save your answers to a JSON file. Failure to enter your name exactly as it appears in Canvas will result in failed grade, as you will be giving the autograder incorrect information with which to record grades. After the JSON file is created, upload it to the assignment page on Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw3_answers.print_answers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignment_name = \"hw_3\"\n",
    "first_name = \"YOUR_FIRST_NAME\" # Use proper capitalization\n",
    "last_name = \"YOUR_LAST_NAME\" # Use proper capitalization\n",
    "\n",
    "hw3_answers.save_to_json(assignment_name, first_name, last_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions?\n",
    "\n",
    "Reach out to your instructors on Piazza.\n",
    "\n",
    "### Sources\n",
    "\n",
    "***\n",
    "\n",
    "<sup>[1]</sup> Sutton, R., & Barto, A. (1998). Introduction to Reinforcement Learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
